{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv3mTq3fAOGO",
        "outputId": "95125c08-26aa-4751-c11e-290ea1316495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "hV3H7_-KBVWE"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-kXLd62CKUE",
        "outputId": "eabed358-35d8-496e-8ac9-8c8c9b6b3642"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FashionMINST dataset"
      ],
      "metadata": {
        "id": "MUQgfJxix65R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP model"
      ],
      "metadata": {
        "id": "1_0Cy6bhrtio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "tr6kaLAND6Kv"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "n_classes = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "n_epochs = 50"
      ],
      "metadata": {
        "id": "TuIq3rP6E7iK"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.FashionMNIST(root='/content/drive/MyDrive/datasets/fashionmnist', train=True,transform=transforms.ToTensor(),download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True)\n",
        "test_dataset = datasets.FashionMNIST(root='/content/drive/MyDrive/datasets/fashionmnist', train=False,transform=transforms.ToTensor(),download=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "metadata": {
        "id": "iInk8duwFxx0"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF6lZ0yZSqHS",
        "outputId": "9c4ddc79-b8b8-4ca8-cdc8-77f48beb541e"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T-shirt/top',\n",
              " 'Trouser',\n",
              " 'Pullover',\n",
              " 'Dress',\n",
              " 'Coat',\n",
              " 'Sandal',\n",
              " 'Shirt',\n",
              " 'Sneaker',\n",
              " 'Bag',\n",
              " 'Ankle boot']"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_dataset[0]\n",
        "plt.imshow(image.squeeze(), cmap='gray')\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "aspo_JhxJXLh",
        "outputId": "71d39133-59c6-4322-d73a-b27792983d1c"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, n_classes):\n",
        "    super().__init__()\n",
        "    self.input_layer = nn.Linear(input_size, 100)\n",
        "    self.hidden_layer_1 = nn.Linear(100,100)\n",
        "    self.hidden_layer_2 = nn.Linear(100,50)\n",
        "    self.hidden_layer_3 = nn.Linear(50,25)\n",
        "    self.output_layer = nn.Linear(25, n_classes)\n",
        "  \n",
        "  def forward(self, X):\n",
        "    X = self.input_layer(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.hidden_layer_1(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.hidden_layer_2(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.hidden_layer_3(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.output_layer(X)\n",
        "    prob = F.softmax(X, dim = 1)\n",
        "    return prob"
      ],
      "metadata": {
        "id": "mSae3WHWVtkT"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(input_size=input_size,n_classes=n_classes).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSuaNo_LWszY",
        "outputId": "5b18d17a-0d6e-4dba-9ebc-46f419d4fe56"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (input_layer): Linear(in_features=784, out_features=100, bias=True)\n",
            "  (hidden_layer_1): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (hidden_layer_2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (hidden_layer_3): Linear(in_features=50, out_features=25, bias=True)\n",
            "  (output_layer): Linear(in_features=25, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "aa-7ABidPv6a"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "  for batch_idx, (data,targets) in enumerate(train_loader):\n",
        "    \n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "\n",
        "    data = data.reshape(data.shape[0],-1)\n",
        "\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch_idx + 1) % 100 == 0:\n",
        "      print(f'Epoch {epoch+1}/{n_epochs}, Batch {batch_idx+1}, Loss: {loss.item():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4MJQ0nKQmkS",
        "outputId": "09cd9cba-85a5-448b-df61-24c7449adf89"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Batch 100, Loss: 1.98\n",
            "Epoch 1/50, Batch 200, Loss: 1.81\n",
            "Epoch 1/50, Batch 300, Loss: 1.81\n",
            "Epoch 1/50, Batch 400, Loss: 1.80\n",
            "Epoch 1/50, Batch 500, Loss: 1.87\n",
            "Epoch 1/50, Batch 600, Loss: 1.83\n",
            "Epoch 1/50, Batch 700, Loss: 1.80\n",
            "Epoch 1/50, Batch 800, Loss: 1.89\n",
            "Epoch 1/50, Batch 900, Loss: 1.76\n",
            "Epoch 2/50, Batch 100, Loss: 1.76\n",
            "Epoch 2/50, Batch 200, Loss: 1.88\n",
            "Epoch 2/50, Batch 300, Loss: 1.84\n",
            "Epoch 2/50, Batch 400, Loss: 1.79\n",
            "Epoch 2/50, Batch 500, Loss: 1.71\n",
            "Epoch 2/50, Batch 600, Loss: 1.71\n",
            "Epoch 2/50, Batch 700, Loss: 1.72\n",
            "Epoch 2/50, Batch 800, Loss: 1.72\n",
            "Epoch 2/50, Batch 900, Loss: 1.64\n",
            "Epoch 3/50, Batch 100, Loss: 1.74\n",
            "Epoch 3/50, Batch 200, Loss: 1.74\n",
            "Epoch 3/50, Batch 300, Loss: 1.71\n",
            "Epoch 3/50, Batch 400, Loss: 1.69\n",
            "Epoch 3/50, Batch 500, Loss: 1.77\n",
            "Epoch 3/50, Batch 600, Loss: 1.60\n",
            "Epoch 3/50, Batch 700, Loss: 1.60\n",
            "Epoch 3/50, Batch 800, Loss: 1.70\n",
            "Epoch 3/50, Batch 900, Loss: 1.66\n",
            "Epoch 4/50, Batch 100, Loss: 1.65\n",
            "Epoch 4/50, Batch 200, Loss: 1.66\n",
            "Epoch 4/50, Batch 300, Loss: 1.73\n",
            "Epoch 4/50, Batch 400, Loss: 1.63\n",
            "Epoch 4/50, Batch 500, Loss: 1.68\n",
            "Epoch 4/50, Batch 600, Loss: 1.57\n",
            "Epoch 4/50, Batch 700, Loss: 1.60\n",
            "Epoch 4/50, Batch 800, Loss: 1.70\n",
            "Epoch 4/50, Batch 900, Loss: 1.57\n",
            "Epoch 5/50, Batch 100, Loss: 1.58\n",
            "Epoch 5/50, Batch 200, Loss: 1.59\n",
            "Epoch 5/50, Batch 300, Loss: 1.67\n",
            "Epoch 5/50, Batch 400, Loss: 1.68\n",
            "Epoch 5/50, Batch 500, Loss: 1.70\n",
            "Epoch 5/50, Batch 600, Loss: 1.69\n",
            "Epoch 5/50, Batch 700, Loss: 1.63\n",
            "Epoch 5/50, Batch 800, Loss: 1.67\n",
            "Epoch 5/50, Batch 900, Loss: 1.64\n",
            "Epoch 6/50, Batch 100, Loss: 1.60\n",
            "Epoch 6/50, Batch 200, Loss: 1.61\n",
            "Epoch 6/50, Batch 300, Loss: 1.59\n",
            "Epoch 6/50, Batch 400, Loss: 1.59\n",
            "Epoch 6/50, Batch 500, Loss: 1.57\n",
            "Epoch 6/50, Batch 600, Loss: 1.58\n",
            "Epoch 6/50, Batch 700, Loss: 1.62\n",
            "Epoch 6/50, Batch 800, Loss: 1.59\n",
            "Epoch 6/50, Batch 900, Loss: 1.68\n",
            "Epoch 7/50, Batch 100, Loss: 1.53\n",
            "Epoch 7/50, Batch 200, Loss: 1.61\n",
            "Epoch 7/50, Batch 300, Loss: 1.65\n",
            "Epoch 7/50, Batch 400, Loss: 1.55\n",
            "Epoch 7/50, Batch 500, Loss: 1.54\n",
            "Epoch 7/50, Batch 600, Loss: 1.55\n",
            "Epoch 7/50, Batch 700, Loss: 1.61\n",
            "Epoch 7/50, Batch 800, Loss: 1.63\n",
            "Epoch 7/50, Batch 900, Loss: 1.56\n",
            "Epoch 8/50, Batch 100, Loss: 1.56\n",
            "Epoch 8/50, Batch 200, Loss: 1.64\n",
            "Epoch 8/50, Batch 300, Loss: 1.58\n",
            "Epoch 8/50, Batch 400, Loss: 1.58\n",
            "Epoch 8/50, Batch 500, Loss: 1.60\n",
            "Epoch 8/50, Batch 600, Loss: 1.61\n",
            "Epoch 8/50, Batch 700, Loss: 1.61\n",
            "Epoch 8/50, Batch 800, Loss: 1.55\n",
            "Epoch 8/50, Batch 900, Loss: 1.63\n",
            "Epoch 9/50, Batch 100, Loss: 1.59\n",
            "Epoch 9/50, Batch 200, Loss: 1.60\n",
            "Epoch 9/50, Batch 300, Loss: 1.56\n",
            "Epoch 9/50, Batch 400, Loss: 1.60\n",
            "Epoch 9/50, Batch 500, Loss: 1.73\n",
            "Epoch 9/50, Batch 600, Loss: 1.67\n",
            "Epoch 9/50, Batch 700, Loss: 1.57\n",
            "Epoch 9/50, Batch 800, Loss: 1.61\n",
            "Epoch 9/50, Batch 900, Loss: 1.60\n",
            "Epoch 10/50, Batch 100, Loss: 1.55\n",
            "Epoch 10/50, Batch 200, Loss: 1.59\n",
            "Epoch 10/50, Batch 300, Loss: 1.52\n",
            "Epoch 10/50, Batch 400, Loss: 1.62\n",
            "Epoch 10/50, Batch 500, Loss: 1.58\n",
            "Epoch 10/50, Batch 600, Loss: 1.63\n",
            "Epoch 10/50, Batch 700, Loss: 1.62\n",
            "Epoch 10/50, Batch 800, Loss: 1.61\n",
            "Epoch 10/50, Batch 900, Loss: 1.57\n",
            "Epoch 11/50, Batch 100, Loss: 1.55\n",
            "Epoch 11/50, Batch 200, Loss: 1.52\n",
            "Epoch 11/50, Batch 300, Loss: 1.55\n",
            "Epoch 11/50, Batch 400, Loss: 1.57\n",
            "Epoch 11/50, Batch 500, Loss: 1.55\n",
            "Epoch 11/50, Batch 600, Loss: 1.64\n",
            "Epoch 11/50, Batch 700, Loss: 1.63\n",
            "Epoch 11/50, Batch 800, Loss: 1.59\n",
            "Epoch 11/50, Batch 900, Loss: 1.64\n",
            "Epoch 12/50, Batch 100, Loss: 1.60\n",
            "Epoch 12/50, Batch 200, Loss: 1.61\n",
            "Epoch 12/50, Batch 300, Loss: 1.58\n",
            "Epoch 12/50, Batch 400, Loss: 1.61\n",
            "Epoch 12/50, Batch 500, Loss: 1.63\n",
            "Epoch 12/50, Batch 600, Loss: 1.58\n",
            "Epoch 12/50, Batch 700, Loss: 1.59\n",
            "Epoch 12/50, Batch 800, Loss: 1.54\n",
            "Epoch 12/50, Batch 900, Loss: 1.55\n",
            "Epoch 13/50, Batch 100, Loss: 1.59\n",
            "Epoch 13/50, Batch 200, Loss: 1.62\n",
            "Epoch 13/50, Batch 300, Loss: 1.63\n",
            "Epoch 13/50, Batch 400, Loss: 1.60\n",
            "Epoch 13/50, Batch 500, Loss: 1.55\n",
            "Epoch 13/50, Batch 600, Loss: 1.57\n",
            "Epoch 13/50, Batch 700, Loss: 1.59\n",
            "Epoch 13/50, Batch 800, Loss: 1.54\n",
            "Epoch 13/50, Batch 900, Loss: 1.54\n",
            "Epoch 14/50, Batch 100, Loss: 1.61\n",
            "Epoch 14/50, Batch 200, Loss: 1.57\n",
            "Epoch 14/50, Batch 300, Loss: 1.60\n",
            "Epoch 14/50, Batch 400, Loss: 1.62\n",
            "Epoch 14/50, Batch 500, Loss: 1.52\n",
            "Epoch 14/50, Batch 600, Loss: 1.58\n",
            "Epoch 14/50, Batch 700, Loss: 1.60\n",
            "Epoch 14/50, Batch 800, Loss: 1.62\n",
            "Epoch 14/50, Batch 900, Loss: 1.63\n",
            "Epoch 15/50, Batch 100, Loss: 1.60\n",
            "Epoch 15/50, Batch 200, Loss: 1.59\n",
            "Epoch 15/50, Batch 300, Loss: 1.59\n",
            "Epoch 15/50, Batch 400, Loss: 1.61\n",
            "Epoch 15/50, Batch 500, Loss: 1.61\n",
            "Epoch 15/50, Batch 600, Loss: 1.55\n",
            "Epoch 15/50, Batch 700, Loss: 1.64\n",
            "Epoch 15/50, Batch 800, Loss: 1.61\n",
            "Epoch 15/50, Batch 900, Loss: 1.60\n",
            "Epoch 16/50, Batch 100, Loss: 1.56\n",
            "Epoch 16/50, Batch 200, Loss: 1.54\n",
            "Epoch 16/50, Batch 300, Loss: 1.58\n",
            "Epoch 16/50, Batch 400, Loss: 1.53\n",
            "Epoch 16/50, Batch 500, Loss: 1.55\n",
            "Epoch 16/50, Batch 600, Loss: 1.57\n",
            "Epoch 16/50, Batch 700, Loss: 1.62\n",
            "Epoch 16/50, Batch 800, Loss: 1.59\n",
            "Epoch 16/50, Batch 900, Loss: 1.55\n",
            "Epoch 17/50, Batch 100, Loss: 1.58\n",
            "Epoch 17/50, Batch 200, Loss: 1.63\n",
            "Epoch 17/50, Batch 300, Loss: 1.51\n",
            "Epoch 17/50, Batch 400, Loss: 1.55\n",
            "Epoch 17/50, Batch 500, Loss: 1.54\n",
            "Epoch 17/50, Batch 600, Loss: 1.56\n",
            "Epoch 17/50, Batch 700, Loss: 1.63\n",
            "Epoch 17/50, Batch 800, Loss: 1.52\n",
            "Epoch 17/50, Batch 900, Loss: 1.62\n",
            "Epoch 18/50, Batch 100, Loss: 1.55\n",
            "Epoch 18/50, Batch 200, Loss: 1.60\n",
            "Epoch 18/50, Batch 300, Loss: 1.60\n",
            "Epoch 18/50, Batch 400, Loss: 1.59\n",
            "Epoch 18/50, Batch 500, Loss: 1.65\n",
            "Epoch 18/50, Batch 600, Loss: 1.60\n",
            "Epoch 18/50, Batch 700, Loss: 1.62\n",
            "Epoch 18/50, Batch 800, Loss: 1.60\n",
            "Epoch 18/50, Batch 900, Loss: 1.51\n",
            "Epoch 19/50, Batch 100, Loss: 1.51\n",
            "Epoch 19/50, Batch 200, Loss: 1.65\n",
            "Epoch 19/50, Batch 300, Loss: 1.59\n",
            "Epoch 19/50, Batch 400, Loss: 1.60\n",
            "Epoch 19/50, Batch 500, Loss: 1.63\n",
            "Epoch 19/50, Batch 600, Loss: 1.58\n",
            "Epoch 19/50, Batch 700, Loss: 1.56\n",
            "Epoch 19/50, Batch 800, Loss: 1.62\n",
            "Epoch 19/50, Batch 900, Loss: 1.62\n",
            "Epoch 20/50, Batch 100, Loss: 1.57\n",
            "Epoch 20/50, Batch 200, Loss: 1.52\n",
            "Epoch 20/50, Batch 300, Loss: 1.56\n",
            "Epoch 20/50, Batch 400, Loss: 1.55\n",
            "Epoch 20/50, Batch 500, Loss: 1.61\n",
            "Epoch 20/50, Batch 600, Loss: 1.64\n",
            "Epoch 20/50, Batch 700, Loss: 1.58\n",
            "Epoch 20/50, Batch 800, Loss: 1.56\n",
            "Epoch 20/50, Batch 900, Loss: 1.60\n",
            "Epoch 21/50, Batch 100, Loss: 1.61\n",
            "Epoch 21/50, Batch 200, Loss: 1.57\n",
            "Epoch 21/50, Batch 300, Loss: 1.60\n",
            "Epoch 21/50, Batch 400, Loss: 1.59\n",
            "Epoch 21/50, Batch 500, Loss: 1.59\n",
            "Epoch 21/50, Batch 600, Loss: 1.58\n",
            "Epoch 21/50, Batch 700, Loss: 1.61\n",
            "Epoch 21/50, Batch 800, Loss: 1.59\n",
            "Epoch 21/50, Batch 900, Loss: 1.57\n",
            "Epoch 22/50, Batch 100, Loss: 1.52\n",
            "Epoch 22/50, Batch 200, Loss: 1.54\n",
            "Epoch 22/50, Batch 300, Loss: 1.66\n",
            "Epoch 22/50, Batch 400, Loss: 1.60\n",
            "Epoch 22/50, Batch 500, Loss: 1.59\n",
            "Epoch 22/50, Batch 600, Loss: 1.63\n",
            "Epoch 22/50, Batch 700, Loss: 1.56\n",
            "Epoch 22/50, Batch 800, Loss: 1.56\n",
            "Epoch 22/50, Batch 900, Loss: 1.62\n",
            "Epoch 23/50, Batch 100, Loss: 1.54\n",
            "Epoch 23/50, Batch 200, Loss: 1.57\n",
            "Epoch 23/50, Batch 300, Loss: 1.57\n",
            "Epoch 23/50, Batch 400, Loss: 1.55\n",
            "Epoch 23/50, Batch 500, Loss: 1.52\n",
            "Epoch 23/50, Batch 600, Loss: 1.67\n",
            "Epoch 23/50, Batch 700, Loss: 1.54\n",
            "Epoch 23/50, Batch 800, Loss: 1.54\n",
            "Epoch 23/50, Batch 900, Loss: 1.59\n",
            "Epoch 24/50, Batch 100, Loss: 1.52\n",
            "Epoch 24/50, Batch 200, Loss: 1.62\n",
            "Epoch 24/50, Batch 300, Loss: 1.51\n",
            "Epoch 24/50, Batch 400, Loss: 1.58\n",
            "Epoch 24/50, Batch 500, Loss: 1.58\n",
            "Epoch 24/50, Batch 600, Loss: 1.63\n",
            "Epoch 24/50, Batch 700, Loss: 1.65\n",
            "Epoch 24/50, Batch 800, Loss: 1.56\n",
            "Epoch 24/50, Batch 900, Loss: 1.63\n",
            "Epoch 25/50, Batch 100, Loss: 1.59\n",
            "Epoch 25/50, Batch 200, Loss: 1.59\n",
            "Epoch 25/50, Batch 300, Loss: 1.59\n",
            "Epoch 25/50, Batch 400, Loss: 1.59\n",
            "Epoch 25/50, Batch 500, Loss: 1.51\n",
            "Epoch 25/50, Batch 600, Loss: 1.56\n",
            "Epoch 25/50, Batch 700, Loss: 1.54\n",
            "Epoch 25/50, Batch 800, Loss: 1.57\n",
            "Epoch 25/50, Batch 900, Loss: 1.61\n",
            "Epoch 26/50, Batch 100, Loss: 1.57\n",
            "Epoch 26/50, Batch 200, Loss: 1.59\n",
            "Epoch 26/50, Batch 300, Loss: 1.61\n",
            "Epoch 26/50, Batch 400, Loss: 1.56\n",
            "Epoch 26/50, Batch 500, Loss: 1.57\n",
            "Epoch 26/50, Batch 600, Loss: 1.62\n",
            "Epoch 26/50, Batch 700, Loss: 1.57\n",
            "Epoch 26/50, Batch 800, Loss: 1.56\n",
            "Epoch 26/50, Batch 900, Loss: 1.55\n",
            "Epoch 27/50, Batch 100, Loss: 1.57\n",
            "Epoch 27/50, Batch 200, Loss: 1.60\n",
            "Epoch 27/50, Batch 300, Loss: 1.63\n",
            "Epoch 27/50, Batch 400, Loss: 1.52\n",
            "Epoch 27/50, Batch 500, Loss: 1.60\n",
            "Epoch 27/50, Batch 600, Loss: 1.56\n",
            "Epoch 27/50, Batch 700, Loss: 1.66\n",
            "Epoch 27/50, Batch 800, Loss: 1.54\n",
            "Epoch 27/50, Batch 900, Loss: 1.67\n",
            "Epoch 28/50, Batch 100, Loss: 1.53\n",
            "Epoch 28/50, Batch 200, Loss: 1.60\n",
            "Epoch 28/50, Batch 300, Loss: 1.61\n",
            "Epoch 28/50, Batch 400, Loss: 1.58\n",
            "Epoch 28/50, Batch 500, Loss: 1.65\n",
            "Epoch 28/50, Batch 600, Loss: 1.62\n",
            "Epoch 28/50, Batch 700, Loss: 1.57\n",
            "Epoch 28/50, Batch 800, Loss: 1.52\n",
            "Epoch 28/50, Batch 900, Loss: 1.57\n",
            "Epoch 29/50, Batch 100, Loss: 1.61\n",
            "Epoch 29/50, Batch 200, Loss: 1.60\n",
            "Epoch 29/50, Batch 300, Loss: 1.64\n",
            "Epoch 29/50, Batch 400, Loss: 1.64\n",
            "Epoch 29/50, Batch 500, Loss: 1.60\n",
            "Epoch 29/50, Batch 600, Loss: 1.54\n",
            "Epoch 29/50, Batch 700, Loss: 1.57\n",
            "Epoch 29/50, Batch 800, Loss: 1.69\n",
            "Epoch 29/50, Batch 900, Loss: 1.56\n",
            "Epoch 30/50, Batch 100, Loss: 1.59\n",
            "Epoch 30/50, Batch 200, Loss: 1.63\n",
            "Epoch 30/50, Batch 300, Loss: 1.52\n",
            "Epoch 30/50, Batch 400, Loss: 1.57\n",
            "Epoch 30/50, Batch 500, Loss: 1.51\n",
            "Epoch 30/50, Batch 600, Loss: 1.62\n",
            "Epoch 30/50, Batch 700, Loss: 1.59\n",
            "Epoch 30/50, Batch 800, Loss: 1.60\n",
            "Epoch 30/50, Batch 900, Loss: 1.66\n",
            "Epoch 31/50, Batch 100, Loss: 1.56\n",
            "Epoch 31/50, Batch 200, Loss: 1.57\n",
            "Epoch 31/50, Batch 300, Loss: 1.58\n",
            "Epoch 31/50, Batch 400, Loss: 1.63\n",
            "Epoch 31/50, Batch 500, Loss: 1.57\n",
            "Epoch 31/50, Batch 600, Loss: 1.61\n",
            "Epoch 31/50, Batch 700, Loss: 1.56\n",
            "Epoch 31/50, Batch 800, Loss: 1.60\n",
            "Epoch 31/50, Batch 900, Loss: 1.65\n",
            "Epoch 32/50, Batch 100, Loss: 1.61\n",
            "Epoch 32/50, Batch 200, Loss: 1.61\n",
            "Epoch 32/50, Batch 300, Loss: 1.62\n",
            "Epoch 32/50, Batch 400, Loss: 1.54\n",
            "Epoch 32/50, Batch 500, Loss: 1.61\n",
            "Epoch 32/50, Batch 600, Loss: 1.57\n",
            "Epoch 32/50, Batch 700, Loss: 1.57\n",
            "Epoch 32/50, Batch 800, Loss: 1.61\n",
            "Epoch 32/50, Batch 900, Loss: 1.62\n",
            "Epoch 33/50, Batch 100, Loss: 1.57\n",
            "Epoch 33/50, Batch 200, Loss: 1.61\n",
            "Epoch 33/50, Batch 300, Loss: 1.54\n",
            "Epoch 33/50, Batch 400, Loss: 1.72\n",
            "Epoch 33/50, Batch 500, Loss: 1.51\n",
            "Epoch 33/50, Batch 600, Loss: 1.57\n",
            "Epoch 33/50, Batch 700, Loss: 1.62\n",
            "Epoch 33/50, Batch 800, Loss: 1.55\n",
            "Epoch 33/50, Batch 900, Loss: 1.57\n",
            "Epoch 34/50, Batch 100, Loss: 1.60\n",
            "Epoch 34/50, Batch 200, Loss: 1.63\n",
            "Epoch 34/50, Batch 300, Loss: 1.54\n",
            "Epoch 34/50, Batch 400, Loss: 1.55\n",
            "Epoch 34/50, Batch 500, Loss: 1.62\n",
            "Epoch 34/50, Batch 600, Loss: 1.57\n",
            "Epoch 34/50, Batch 700, Loss: 1.60\n",
            "Epoch 34/50, Batch 800, Loss: 1.65\n",
            "Epoch 34/50, Batch 900, Loss: 1.54\n",
            "Epoch 35/50, Batch 100, Loss: 1.52\n",
            "Epoch 35/50, Batch 200, Loss: 1.57\n",
            "Epoch 35/50, Batch 300, Loss: 1.59\n",
            "Epoch 35/50, Batch 400, Loss: 1.60\n",
            "Epoch 35/50, Batch 500, Loss: 1.55\n",
            "Epoch 35/50, Batch 600, Loss: 1.63\n",
            "Epoch 35/50, Batch 700, Loss: 1.60\n",
            "Epoch 35/50, Batch 800, Loss: 1.61\n",
            "Epoch 35/50, Batch 900, Loss: 1.62\n",
            "Epoch 36/50, Batch 100, Loss: 1.55\n",
            "Epoch 36/50, Batch 200, Loss: 1.60\n",
            "Epoch 36/50, Batch 300, Loss: 1.60\n",
            "Epoch 36/50, Batch 400, Loss: 1.55\n",
            "Epoch 36/50, Batch 500, Loss: 1.57\n",
            "Epoch 36/50, Batch 600, Loss: 1.62\n",
            "Epoch 36/50, Batch 700, Loss: 1.59\n",
            "Epoch 36/50, Batch 800, Loss: 1.61\n",
            "Epoch 36/50, Batch 900, Loss: 1.62\n",
            "Epoch 37/50, Batch 100, Loss: 1.60\n",
            "Epoch 37/50, Batch 200, Loss: 1.62\n",
            "Epoch 37/50, Batch 300, Loss: 1.63\n",
            "Epoch 37/50, Batch 400, Loss: 1.54\n",
            "Epoch 37/50, Batch 500, Loss: 1.54\n",
            "Epoch 37/50, Batch 600, Loss: 1.63\n",
            "Epoch 37/50, Batch 700, Loss: 1.53\n",
            "Epoch 37/50, Batch 800, Loss: 1.60\n",
            "Epoch 37/50, Batch 900, Loss: 1.60\n",
            "Epoch 38/50, Batch 100, Loss: 1.59\n",
            "Epoch 38/50, Batch 200, Loss: 1.57\n",
            "Epoch 38/50, Batch 300, Loss: 1.59\n",
            "Epoch 38/50, Batch 400, Loss: 1.59\n",
            "Epoch 38/50, Batch 500, Loss: 1.56\n",
            "Epoch 38/50, Batch 600, Loss: 1.57\n",
            "Epoch 38/50, Batch 700, Loss: 1.57\n",
            "Epoch 38/50, Batch 800, Loss: 1.61\n",
            "Epoch 38/50, Batch 900, Loss: 1.55\n",
            "Epoch 39/50, Batch 100, Loss: 1.57\n",
            "Epoch 39/50, Batch 200, Loss: 1.60\n",
            "Epoch 39/50, Batch 300, Loss: 1.62\n",
            "Epoch 39/50, Batch 400, Loss: 1.57\n",
            "Epoch 39/50, Batch 500, Loss: 1.54\n",
            "Epoch 39/50, Batch 600, Loss: 1.59\n",
            "Epoch 39/50, Batch 700, Loss: 1.55\n",
            "Epoch 39/50, Batch 800, Loss: 1.59\n",
            "Epoch 39/50, Batch 900, Loss: 1.54\n",
            "Epoch 40/50, Batch 100, Loss: 1.60\n",
            "Epoch 40/50, Batch 200, Loss: 1.64\n",
            "Epoch 40/50, Batch 300, Loss: 1.57\n",
            "Epoch 40/50, Batch 400, Loss: 1.62\n",
            "Epoch 40/50, Batch 500, Loss: 1.57\n",
            "Epoch 40/50, Batch 600, Loss: 1.59\n",
            "Epoch 40/50, Batch 700, Loss: 1.65\n",
            "Epoch 40/50, Batch 800, Loss: 1.61\n",
            "Epoch 40/50, Batch 900, Loss: 1.63\n",
            "Epoch 41/50, Batch 100, Loss: 1.62\n",
            "Epoch 41/50, Batch 200, Loss: 1.57\n",
            "Epoch 41/50, Batch 300, Loss: 1.56\n",
            "Epoch 41/50, Batch 400, Loss: 1.52\n",
            "Epoch 41/50, Batch 500, Loss: 1.59\n",
            "Epoch 41/50, Batch 600, Loss: 1.58\n",
            "Epoch 41/50, Batch 700, Loss: 1.62\n",
            "Epoch 41/50, Batch 800, Loss: 1.62\n",
            "Epoch 41/50, Batch 900, Loss: 1.52\n",
            "Epoch 42/50, Batch 100, Loss: 1.57\n",
            "Epoch 42/50, Batch 200, Loss: 1.54\n",
            "Epoch 42/50, Batch 300, Loss: 1.55\n",
            "Epoch 42/50, Batch 400, Loss: 1.63\n",
            "Epoch 42/50, Batch 500, Loss: 1.59\n",
            "Epoch 42/50, Batch 600, Loss: 1.59\n",
            "Epoch 42/50, Batch 700, Loss: 1.60\n",
            "Epoch 42/50, Batch 800, Loss: 1.51\n",
            "Epoch 42/50, Batch 900, Loss: 1.57\n",
            "Epoch 43/50, Batch 100, Loss: 1.59\n",
            "Epoch 43/50, Batch 200, Loss: 1.52\n",
            "Epoch 43/50, Batch 300, Loss: 1.56\n",
            "Epoch 43/50, Batch 400, Loss: 1.61\n",
            "Epoch 43/50, Batch 500, Loss: 1.53\n",
            "Epoch 43/50, Batch 600, Loss: 1.52\n",
            "Epoch 43/50, Batch 700, Loss: 1.66\n",
            "Epoch 43/50, Batch 800, Loss: 1.58\n",
            "Epoch 43/50, Batch 900, Loss: 1.57\n",
            "Epoch 44/50, Batch 100, Loss: 1.62\n",
            "Epoch 44/50, Batch 200, Loss: 1.58\n",
            "Epoch 44/50, Batch 300, Loss: 1.60\n",
            "Epoch 44/50, Batch 400, Loss: 1.58\n",
            "Epoch 44/50, Batch 500, Loss: 1.54\n",
            "Epoch 44/50, Batch 600, Loss: 1.54\n",
            "Epoch 44/50, Batch 700, Loss: 1.54\n",
            "Epoch 44/50, Batch 800, Loss: 1.60\n",
            "Epoch 44/50, Batch 900, Loss: 1.59\n",
            "Epoch 45/50, Batch 100, Loss: 1.66\n",
            "Epoch 45/50, Batch 200, Loss: 1.59\n",
            "Epoch 45/50, Batch 300, Loss: 1.56\n",
            "Epoch 45/50, Batch 400, Loss: 1.59\n",
            "Epoch 45/50, Batch 500, Loss: 1.51\n",
            "Epoch 45/50, Batch 600, Loss: 1.60\n",
            "Epoch 45/50, Batch 700, Loss: 1.65\n",
            "Epoch 45/50, Batch 800, Loss: 1.52\n",
            "Epoch 45/50, Batch 900, Loss: 1.55\n",
            "Epoch 46/50, Batch 100, Loss: 1.57\n",
            "Epoch 46/50, Batch 200, Loss: 1.60\n",
            "Epoch 46/50, Batch 300, Loss: 1.57\n",
            "Epoch 46/50, Batch 400, Loss: 1.58\n",
            "Epoch 46/50, Batch 500, Loss: 1.57\n",
            "Epoch 46/50, Batch 600, Loss: 1.59\n",
            "Epoch 46/50, Batch 700, Loss: 1.54\n",
            "Epoch 46/50, Batch 800, Loss: 1.59\n",
            "Epoch 46/50, Batch 900, Loss: 1.62\n",
            "Epoch 47/50, Batch 100, Loss: 1.63\n",
            "Epoch 47/50, Batch 200, Loss: 1.57\n",
            "Epoch 47/50, Batch 300, Loss: 1.73\n",
            "Epoch 47/50, Batch 400, Loss: 1.66\n",
            "Epoch 47/50, Batch 500, Loss: 1.51\n",
            "Epoch 47/50, Batch 600, Loss: 1.62\n",
            "Epoch 47/50, Batch 700, Loss: 1.62\n",
            "Epoch 47/50, Batch 800, Loss: 1.57\n",
            "Epoch 47/50, Batch 900, Loss: 1.57\n",
            "Epoch 48/50, Batch 100, Loss: 1.54\n",
            "Epoch 48/50, Batch 200, Loss: 1.55\n",
            "Epoch 48/50, Batch 300, Loss: 1.62\n",
            "Epoch 48/50, Batch 400, Loss: 1.60\n",
            "Epoch 48/50, Batch 500, Loss: 1.62\n",
            "Epoch 48/50, Batch 600, Loss: 1.55\n",
            "Epoch 48/50, Batch 700, Loss: 1.62\n",
            "Epoch 48/50, Batch 800, Loss: 1.61\n",
            "Epoch 48/50, Batch 900, Loss: 1.55\n",
            "Epoch 49/50, Batch 100, Loss: 1.61\n",
            "Epoch 49/50, Batch 200, Loss: 1.51\n",
            "Epoch 49/50, Batch 300, Loss: 1.59\n",
            "Epoch 49/50, Batch 400, Loss: 1.60\n",
            "Epoch 49/50, Batch 500, Loss: 1.70\n",
            "Epoch 49/50, Batch 600, Loss: 1.52\n",
            "Epoch 49/50, Batch 700, Loss: 1.54\n",
            "Epoch 49/50, Batch 800, Loss: 1.61\n",
            "Epoch 49/50, Batch 900, Loss: 1.49\n",
            "Epoch 50/50, Batch 100, Loss: 1.57\n",
            "Epoch 50/50, Batch 200, Loss: 1.55\n",
            "Epoch 50/50, Batch 300, Loss: 1.63\n",
            "Epoch 50/50, Batch 400, Loss: 1.55\n",
            "Epoch 50/50, Batch 500, Loss: 1.57\n",
            "Epoch 50/50, Batch 600, Loss: 1.59\n",
            "Epoch 50/50, Batch 700, Loss: 1.59\n",
            "Epoch 50/50, Batch 800, Loss: 1.60\n",
            "Epoch 50/50, Batch 900, Loss: 1.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(loader,model):\n",
        "  if loader.dataset.train:\n",
        "    print('Getting accuracy on training data.')\n",
        "  else:\n",
        "    print('Getting accuracy on testing data.')\n",
        "  n_corrects = 0\n",
        "  n_samples = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      x = x.reshape(x.shape[0],-1)\n",
        "\n",
        "      scores = model(x)\n",
        "      _,y_pred = scores.max(1)\n",
        "      n_corrects += (y_pred == y).sum()\n",
        "      n_samples += y_pred.size(0)\n",
        "\n",
        "    print(f'We got {n_corrects}/{n_samples} correct. Accuracy = {float(n_corrects)/float(n_samples)*100.0:.2f}')\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "Ym-xlnhyXBi3"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(train_loader, model)\n",
        "get_accuracy(test_loader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXKgv34jYu6W",
        "outputId": "44d9eac8-c5af-47a9-f241-22f56374c7b8"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting accuracy on training data.\n",
            "We got 52798/60000 correct. Accuracy = 88.00\n",
            "Getting accuracy on testing data.\n",
            "We got 8586/10000 correct. Accuracy = 85.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LeNet-5**"
      ],
      "metadata": {
        "id": "tMzYoY4MdriG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "3TnK-5dzZdy6"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "n_epochs = 50"
      ],
      "metadata": {
        "id": "VV5mKSCHZhKu"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = transforms.Compose([transforms.Resize((32,32)), transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "Jnoy2wlaZohc"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.FashionMNIST(root='/content/drive/MyDrive/datasets/mnist', train=True,transform=transforms,download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True)\n",
        "test_dataset = datasets.FashionMNIST(root='/content/drive/MyDrive/datasets/mnist', train=False,transform=transforms,download=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "metadata": {
        "id": "FOBlaQ8cZ4Gf"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LaNet5(nn.Module):\n",
        "  def __init__(self, n_classes):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=6,kernel_size=5,stride=1),\n",
        "        nn.Tanh(),\n",
        "        nn.AvgPool2d(kernel_size=2),\n",
        "        nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5,stride=1),\n",
        "        nn.Tanh(),\n",
        "        nn.AvgPool2d(kernel_size=2),\n",
        "        nn.Conv2d(in_channels=16,out_channels=120,kernel_size=5,stride=1),\n",
        "        nn.Tanh(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=120,out_features=84),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(in_features=84,out_features=n_classes),\n",
        "        nn.Softmax(dim=1)\n",
        "        )\n",
        "  def forward(self, X):\n",
        "    prob = self.model(X)\n",
        "    return prob"
      ],
      "metadata": {
        "id": "-SIj2hTcaDM9"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LaNet5(n_classes=n_classes).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "257FGpETbeA1",
        "outputId": "ef6bce16-e88f-478b-ba4a-ebbd2969fcb7"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LaNet5(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Tanh()\n",
            "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): Tanh()\n",
            "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (6): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (7): Tanh()\n",
            "    (8): Flatten(start_dim=1, end_dim=-1)\n",
            "    (9): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (10): Tanh()\n",
            "    (11): Linear(in_features=84, out_features=10, bias=True)\n",
            "    (12): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "mngMKP1_dZuF"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "  for batch_idx, (data,targets) in enumerate(train_loader):\n",
        "    \n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "\n",
        "    #data = data.reshape(data.shape[0],-1)\n",
        "\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch_idx + 1) % 100 == 0:\n",
        "      print(f'Epoch {epoch+1}/{n_epochs}, Batch {batch_idx+1}, Loss: {loss.item():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RfzIbCkdkLn",
        "outputId": "f82b84f9-6e21-4dc3-e5b7-dae11af6e821"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Batch 100, Loss: 1.86\n",
            "Epoch 1/50, Batch 200, Loss: 1.74\n",
            "Epoch 1/50, Batch 300, Loss: 1.75\n",
            "Epoch 1/50, Batch 400, Loss: 1.61\n",
            "Epoch 1/50, Batch 500, Loss: 1.78\n",
            "Epoch 1/50, Batch 600, Loss: 1.67\n",
            "Epoch 1/50, Batch 700, Loss: 1.72\n",
            "Epoch 1/50, Batch 800, Loss: 1.64\n",
            "Epoch 1/50, Batch 900, Loss: 1.63\n",
            "Epoch 2/50, Batch 100, Loss: 1.62\n",
            "Epoch 2/50, Batch 200, Loss: 1.64\n",
            "Epoch 2/50, Batch 300, Loss: 1.64\n",
            "Epoch 2/50, Batch 400, Loss: 1.67\n",
            "Epoch 2/50, Batch 500, Loss: 1.62\n",
            "Epoch 2/50, Batch 600, Loss: 1.60\n",
            "Epoch 2/50, Batch 700, Loss: 1.61\n",
            "Epoch 2/50, Batch 800, Loss: 1.65\n",
            "Epoch 2/50, Batch 900, Loss: 1.60\n",
            "Epoch 3/50, Batch 100, Loss: 1.61\n",
            "Epoch 3/50, Batch 200, Loss: 1.60\n",
            "Epoch 3/50, Batch 300, Loss: 1.59\n",
            "Epoch 3/50, Batch 400, Loss: 1.60\n",
            "Epoch 3/50, Batch 500, Loss: 1.64\n",
            "Epoch 3/50, Batch 600, Loss: 1.58\n",
            "Epoch 3/50, Batch 700, Loss: 1.64\n",
            "Epoch 3/50, Batch 800, Loss: 1.58\n",
            "Epoch 3/50, Batch 900, Loss: 1.59\n",
            "Epoch 4/50, Batch 100, Loss: 1.69\n",
            "Epoch 4/50, Batch 200, Loss: 1.55\n",
            "Epoch 4/50, Batch 300, Loss: 1.69\n",
            "Epoch 4/50, Batch 400, Loss: 1.60\n",
            "Epoch 4/50, Batch 500, Loss: 1.63\n",
            "Epoch 4/50, Batch 600, Loss: 1.61\n",
            "Epoch 4/50, Batch 700, Loss: 1.57\n",
            "Epoch 4/50, Batch 800, Loss: 1.56\n",
            "Epoch 4/50, Batch 900, Loss: 1.64\n",
            "Epoch 5/50, Batch 100, Loss: 1.56\n",
            "Epoch 5/50, Batch 200, Loss: 1.57\n",
            "Epoch 5/50, Batch 300, Loss: 1.58\n",
            "Epoch 5/50, Batch 400, Loss: 1.55\n",
            "Epoch 5/50, Batch 500, Loss: 1.56\n",
            "Epoch 5/50, Batch 600, Loss: 1.67\n",
            "Epoch 5/50, Batch 700, Loss: 1.66\n",
            "Epoch 5/50, Batch 800, Loss: 1.59\n",
            "Epoch 5/50, Batch 900, Loss: 1.58\n",
            "Epoch 6/50, Batch 100, Loss: 1.62\n",
            "Epoch 6/50, Batch 200, Loss: 1.62\n",
            "Epoch 6/50, Batch 300, Loss: 1.61\n",
            "Epoch 6/50, Batch 400, Loss: 1.58\n",
            "Epoch 6/50, Batch 500, Loss: 1.71\n",
            "Epoch 6/50, Batch 600, Loss: 1.57\n",
            "Epoch 6/50, Batch 700, Loss: 1.63\n",
            "Epoch 6/50, Batch 800, Loss: 1.58\n",
            "Epoch 6/50, Batch 900, Loss: 1.56\n",
            "Epoch 7/50, Batch 100, Loss: 1.67\n",
            "Epoch 7/50, Batch 200, Loss: 1.66\n",
            "Epoch 7/50, Batch 300, Loss: 1.58\n",
            "Epoch 7/50, Batch 400, Loss: 1.64\n",
            "Epoch 7/50, Batch 500, Loss: 1.57\n",
            "Epoch 7/50, Batch 600, Loss: 1.67\n",
            "Epoch 7/50, Batch 700, Loss: 1.50\n",
            "Epoch 7/50, Batch 800, Loss: 1.57\n",
            "Epoch 7/50, Batch 900, Loss: 1.63\n",
            "Epoch 8/50, Batch 100, Loss: 1.54\n",
            "Epoch 8/50, Batch 200, Loss: 1.56\n",
            "Epoch 8/50, Batch 300, Loss: 1.54\n",
            "Epoch 8/50, Batch 400, Loss: 1.57\n",
            "Epoch 8/50, Batch 500, Loss: 1.61\n",
            "Epoch 8/50, Batch 600, Loss: 1.58\n",
            "Epoch 8/50, Batch 700, Loss: 1.64\n",
            "Epoch 8/50, Batch 800, Loss: 1.55\n",
            "Epoch 8/50, Batch 900, Loss: 1.54\n",
            "Epoch 9/50, Batch 100, Loss: 1.59\n",
            "Epoch 9/50, Batch 200, Loss: 1.56\n",
            "Epoch 9/50, Batch 300, Loss: 1.51\n",
            "Epoch 9/50, Batch 400, Loss: 1.57\n",
            "Epoch 9/50, Batch 500, Loss: 1.55\n",
            "Epoch 9/50, Batch 600, Loss: 1.53\n",
            "Epoch 9/50, Batch 700, Loss: 1.62\n",
            "Epoch 9/50, Batch 800, Loss: 1.72\n",
            "Epoch 9/50, Batch 900, Loss: 1.58\n",
            "Epoch 10/50, Batch 100, Loss: 1.56\n",
            "Epoch 10/50, Batch 200, Loss: 1.57\n",
            "Epoch 10/50, Batch 300, Loss: 1.53\n",
            "Epoch 10/50, Batch 400, Loss: 1.51\n",
            "Epoch 10/50, Batch 500, Loss: 1.60\n",
            "Epoch 10/50, Batch 600, Loss: 1.62\n",
            "Epoch 10/50, Batch 700, Loss: 1.58\n",
            "Epoch 10/50, Batch 800, Loss: 1.60\n",
            "Epoch 10/50, Batch 900, Loss: 1.56\n",
            "Epoch 11/50, Batch 100, Loss: 1.53\n",
            "Epoch 11/50, Batch 200, Loss: 1.54\n",
            "Epoch 11/50, Batch 300, Loss: 1.61\n",
            "Epoch 11/50, Batch 400, Loss: 1.56\n",
            "Epoch 11/50, Batch 500, Loss: 1.56\n",
            "Epoch 11/50, Batch 600, Loss: 1.60\n",
            "Epoch 11/50, Batch 700, Loss: 1.61\n",
            "Epoch 11/50, Batch 800, Loss: 1.59\n",
            "Epoch 11/50, Batch 900, Loss: 1.56\n",
            "Epoch 12/50, Batch 100, Loss: 1.61\n",
            "Epoch 12/50, Batch 200, Loss: 1.54\n",
            "Epoch 12/50, Batch 300, Loss: 1.57\n",
            "Epoch 12/50, Batch 400, Loss: 1.52\n",
            "Epoch 12/50, Batch 500, Loss: 1.58\n",
            "Epoch 12/50, Batch 600, Loss: 1.64\n",
            "Epoch 12/50, Batch 700, Loss: 1.51\n",
            "Epoch 12/50, Batch 800, Loss: 1.52\n",
            "Epoch 12/50, Batch 900, Loss: 1.54\n",
            "Epoch 13/50, Batch 100, Loss: 1.61\n",
            "Epoch 13/50, Batch 200, Loss: 1.51\n",
            "Epoch 13/50, Batch 300, Loss: 1.53\n",
            "Epoch 13/50, Batch 400, Loss: 1.56\n",
            "Epoch 13/50, Batch 500, Loss: 1.56\n",
            "Epoch 13/50, Batch 600, Loss: 1.53\n",
            "Epoch 13/50, Batch 700, Loss: 1.57\n",
            "Epoch 13/50, Batch 800, Loss: 1.53\n",
            "Epoch 13/50, Batch 900, Loss: 1.62\n",
            "Epoch 14/50, Batch 100, Loss: 1.56\n",
            "Epoch 14/50, Batch 200, Loss: 1.56\n",
            "Epoch 14/50, Batch 300, Loss: 1.67\n",
            "Epoch 14/50, Batch 400, Loss: 1.57\n",
            "Epoch 14/50, Batch 500, Loss: 1.52\n",
            "Epoch 14/50, Batch 600, Loss: 1.53\n",
            "Epoch 14/50, Batch 700, Loss: 1.56\n",
            "Epoch 14/50, Batch 800, Loss: 1.53\n",
            "Epoch 14/50, Batch 900, Loss: 1.56\n",
            "Epoch 15/50, Batch 100, Loss: 1.57\n",
            "Epoch 15/50, Batch 200, Loss: 1.60\n",
            "Epoch 15/50, Batch 300, Loss: 1.60\n",
            "Epoch 15/50, Batch 400, Loss: 1.53\n",
            "Epoch 15/50, Batch 500, Loss: 1.57\n",
            "Epoch 15/50, Batch 600, Loss: 1.57\n",
            "Epoch 15/50, Batch 700, Loss: 1.57\n",
            "Epoch 15/50, Batch 800, Loss: 1.58\n",
            "Epoch 15/50, Batch 900, Loss: 1.64\n",
            "Epoch 16/50, Batch 100, Loss: 1.52\n",
            "Epoch 16/50, Batch 200, Loss: 1.55\n",
            "Epoch 16/50, Batch 300, Loss: 1.54\n",
            "Epoch 16/50, Batch 400, Loss: 1.58\n",
            "Epoch 16/50, Batch 500, Loss: 1.57\n",
            "Epoch 16/50, Batch 600, Loss: 1.60\n",
            "Epoch 16/50, Batch 700, Loss: 1.62\n",
            "Epoch 16/50, Batch 800, Loss: 1.52\n",
            "Epoch 16/50, Batch 900, Loss: 1.59\n",
            "Epoch 17/50, Batch 100, Loss: 1.53\n",
            "Epoch 17/50, Batch 200, Loss: 1.49\n",
            "Epoch 17/50, Batch 300, Loss: 1.59\n",
            "Epoch 17/50, Batch 400, Loss: 1.53\n",
            "Epoch 17/50, Batch 500, Loss: 1.63\n",
            "Epoch 17/50, Batch 600, Loss: 1.55\n",
            "Epoch 17/50, Batch 700, Loss: 1.54\n",
            "Epoch 17/50, Batch 800, Loss: 1.60\n",
            "Epoch 17/50, Batch 900, Loss: 1.50\n",
            "Epoch 18/50, Batch 100, Loss: 1.50\n",
            "Epoch 18/50, Batch 200, Loss: 1.56\n",
            "Epoch 18/50, Batch 300, Loss: 1.54\n",
            "Epoch 18/50, Batch 400, Loss: 1.65\n",
            "Epoch 18/50, Batch 500, Loss: 1.62\n",
            "Epoch 18/50, Batch 600, Loss: 1.53\n",
            "Epoch 18/50, Batch 700, Loss: 1.50\n",
            "Epoch 18/50, Batch 800, Loss: 1.57\n",
            "Epoch 18/50, Batch 900, Loss: 1.64\n",
            "Epoch 19/50, Batch 100, Loss: 1.53\n",
            "Epoch 19/50, Batch 200, Loss: 1.55\n",
            "Epoch 19/50, Batch 300, Loss: 1.56\n",
            "Epoch 19/50, Batch 400, Loss: 1.55\n",
            "Epoch 19/50, Batch 500, Loss: 1.56\n",
            "Epoch 19/50, Batch 600, Loss: 1.55\n",
            "Epoch 19/50, Batch 700, Loss: 1.54\n",
            "Epoch 19/50, Batch 800, Loss: 1.52\n",
            "Epoch 19/50, Batch 900, Loss: 1.55\n",
            "Epoch 20/50, Batch 100, Loss: 1.60\n",
            "Epoch 20/50, Batch 200, Loss: 1.56\n",
            "Epoch 20/50, Batch 300, Loss: 1.57\n",
            "Epoch 20/50, Batch 400, Loss: 1.55\n",
            "Epoch 20/50, Batch 500, Loss: 1.52\n",
            "Epoch 20/50, Batch 600, Loss: 1.60\n",
            "Epoch 20/50, Batch 700, Loss: 1.57\n",
            "Epoch 20/50, Batch 800, Loss: 1.56\n",
            "Epoch 20/50, Batch 900, Loss: 1.59\n",
            "Epoch 21/50, Batch 100, Loss: 1.55\n",
            "Epoch 21/50, Batch 200, Loss: 1.56\n",
            "Epoch 21/50, Batch 300, Loss: 1.54\n",
            "Epoch 21/50, Batch 400, Loss: 1.55\n",
            "Epoch 21/50, Batch 500, Loss: 1.56\n",
            "Epoch 21/50, Batch 600, Loss: 1.55\n",
            "Epoch 21/50, Batch 700, Loss: 1.53\n",
            "Epoch 21/50, Batch 800, Loss: 1.56\n",
            "Epoch 21/50, Batch 900, Loss: 1.51\n",
            "Epoch 22/50, Batch 100, Loss: 1.53\n",
            "Epoch 22/50, Batch 200, Loss: 1.52\n",
            "Epoch 22/50, Batch 300, Loss: 1.51\n",
            "Epoch 22/50, Batch 400, Loss: 1.54\n",
            "Epoch 22/50, Batch 500, Loss: 1.56\n",
            "Epoch 22/50, Batch 600, Loss: 1.58\n",
            "Epoch 22/50, Batch 700, Loss: 1.59\n",
            "Epoch 22/50, Batch 800, Loss: 1.54\n",
            "Epoch 22/50, Batch 900, Loss: 1.60\n",
            "Epoch 23/50, Batch 100, Loss: 1.52\n",
            "Epoch 23/50, Batch 200, Loss: 1.54\n",
            "Epoch 23/50, Batch 300, Loss: 1.54\n",
            "Epoch 23/50, Batch 400, Loss: 1.62\n",
            "Epoch 23/50, Batch 500, Loss: 1.56\n",
            "Epoch 23/50, Batch 600, Loss: 1.62\n",
            "Epoch 23/50, Batch 700, Loss: 1.56\n",
            "Epoch 23/50, Batch 800, Loss: 1.59\n",
            "Epoch 23/50, Batch 900, Loss: 1.53\n",
            "Epoch 24/50, Batch 100, Loss: 1.56\n",
            "Epoch 24/50, Batch 200, Loss: 1.50\n",
            "Epoch 24/50, Batch 300, Loss: 1.62\n",
            "Epoch 24/50, Batch 400, Loss: 1.59\n",
            "Epoch 24/50, Batch 500, Loss: 1.50\n",
            "Epoch 24/50, Batch 600, Loss: 1.55\n",
            "Epoch 24/50, Batch 700, Loss: 1.54\n",
            "Epoch 24/50, Batch 800, Loss: 1.57\n",
            "Epoch 24/50, Batch 900, Loss: 1.54\n",
            "Epoch 25/50, Batch 100, Loss: 1.55\n",
            "Epoch 25/50, Batch 200, Loss: 1.57\n",
            "Epoch 25/50, Batch 300, Loss: 1.51\n",
            "Epoch 25/50, Batch 400, Loss: 1.61\n",
            "Epoch 25/50, Batch 500, Loss: 1.53\n",
            "Epoch 25/50, Batch 600, Loss: 1.55\n",
            "Epoch 25/50, Batch 700, Loss: 1.51\n",
            "Epoch 25/50, Batch 800, Loss: 1.54\n",
            "Epoch 25/50, Batch 900, Loss: 1.52\n",
            "Epoch 26/50, Batch 100, Loss: 1.51\n",
            "Epoch 26/50, Batch 200, Loss: 1.52\n",
            "Epoch 26/50, Batch 300, Loss: 1.55\n",
            "Epoch 26/50, Batch 400, Loss: 1.55\n",
            "Epoch 26/50, Batch 500, Loss: 1.50\n",
            "Epoch 26/50, Batch 600, Loss: 1.60\n",
            "Epoch 26/50, Batch 700, Loss: 1.57\n",
            "Epoch 26/50, Batch 800, Loss: 1.53\n",
            "Epoch 26/50, Batch 900, Loss: 1.54\n",
            "Epoch 27/50, Batch 100, Loss: 1.58\n",
            "Epoch 27/50, Batch 200, Loss: 1.64\n",
            "Epoch 27/50, Batch 300, Loss: 1.60\n",
            "Epoch 27/50, Batch 400, Loss: 1.56\n",
            "Epoch 27/50, Batch 500, Loss: 1.54\n",
            "Epoch 27/50, Batch 600, Loss: 1.57\n",
            "Epoch 27/50, Batch 700, Loss: 1.59\n",
            "Epoch 27/50, Batch 800, Loss: 1.53\n",
            "Epoch 27/50, Batch 900, Loss: 1.55\n",
            "Epoch 28/50, Batch 100, Loss: 1.48\n",
            "Epoch 28/50, Batch 200, Loss: 1.55\n",
            "Epoch 28/50, Batch 300, Loss: 1.56\n",
            "Epoch 28/50, Batch 400, Loss: 1.52\n",
            "Epoch 28/50, Batch 500, Loss: 1.61\n",
            "Epoch 28/50, Batch 600, Loss: 1.53\n",
            "Epoch 28/50, Batch 700, Loss: 1.50\n",
            "Epoch 28/50, Batch 800, Loss: 1.52\n",
            "Epoch 28/50, Batch 900, Loss: 1.55\n",
            "Epoch 29/50, Batch 100, Loss: 1.55\n",
            "Epoch 29/50, Batch 200, Loss: 1.50\n",
            "Epoch 29/50, Batch 300, Loss: 1.54\n",
            "Epoch 29/50, Batch 400, Loss: 1.51\n",
            "Epoch 29/50, Batch 500, Loss: 1.51\n",
            "Epoch 29/50, Batch 600, Loss: 1.55\n",
            "Epoch 29/50, Batch 700, Loss: 1.55\n",
            "Epoch 29/50, Batch 800, Loss: 1.49\n",
            "Epoch 29/50, Batch 900, Loss: 1.56\n",
            "Epoch 30/50, Batch 100, Loss: 1.56\n",
            "Epoch 30/50, Batch 200, Loss: 1.56\n",
            "Epoch 30/50, Batch 300, Loss: 1.50\n",
            "Epoch 30/50, Batch 400, Loss: 1.58\n",
            "Epoch 30/50, Batch 500, Loss: 1.58\n",
            "Epoch 30/50, Batch 600, Loss: 1.56\n",
            "Epoch 30/50, Batch 700, Loss: 1.52\n",
            "Epoch 30/50, Batch 800, Loss: 1.61\n",
            "Epoch 30/50, Batch 900, Loss: 1.60\n",
            "Epoch 31/50, Batch 100, Loss: 1.51\n",
            "Epoch 31/50, Batch 200, Loss: 1.53\n",
            "Epoch 31/50, Batch 300, Loss: 1.49\n",
            "Epoch 31/50, Batch 400, Loss: 1.59\n",
            "Epoch 31/50, Batch 500, Loss: 1.59\n",
            "Epoch 31/50, Batch 600, Loss: 1.59\n",
            "Epoch 31/50, Batch 700, Loss: 1.59\n",
            "Epoch 31/50, Batch 800, Loss: 1.55\n",
            "Epoch 31/50, Batch 900, Loss: 1.57\n",
            "Epoch 32/50, Batch 100, Loss: 1.56\n",
            "Epoch 32/50, Batch 200, Loss: 1.57\n",
            "Epoch 32/50, Batch 300, Loss: 1.49\n",
            "Epoch 32/50, Batch 400, Loss: 1.50\n",
            "Epoch 32/50, Batch 500, Loss: 1.54\n",
            "Epoch 32/50, Batch 600, Loss: 1.59\n",
            "Epoch 32/50, Batch 700, Loss: 1.55\n",
            "Epoch 32/50, Batch 800, Loss: 1.59\n",
            "Epoch 32/50, Batch 900, Loss: 1.50\n",
            "Epoch 33/50, Batch 100, Loss: 1.54\n",
            "Epoch 33/50, Batch 200, Loss: 1.53\n",
            "Epoch 33/50, Batch 300, Loss: 1.58\n",
            "Epoch 33/50, Batch 400, Loss: 1.51\n",
            "Epoch 33/50, Batch 500, Loss: 1.52\n",
            "Epoch 33/50, Batch 600, Loss: 1.56\n",
            "Epoch 33/50, Batch 700, Loss: 1.53\n",
            "Epoch 33/50, Batch 800, Loss: 1.50\n",
            "Epoch 33/50, Batch 900, Loss: 1.54\n",
            "Epoch 34/50, Batch 100, Loss: 1.56\n",
            "Epoch 34/50, Batch 200, Loss: 1.51\n",
            "Epoch 34/50, Batch 300, Loss: 1.47\n",
            "Epoch 34/50, Batch 400, Loss: 1.50\n",
            "Epoch 34/50, Batch 500, Loss: 1.54\n",
            "Epoch 34/50, Batch 600, Loss: 1.57\n",
            "Epoch 34/50, Batch 700, Loss: 1.53\n",
            "Epoch 34/50, Batch 800, Loss: 1.53\n",
            "Epoch 34/50, Batch 900, Loss: 1.51\n",
            "Epoch 35/50, Batch 100, Loss: 1.50\n",
            "Epoch 35/50, Batch 200, Loss: 1.51\n",
            "Epoch 35/50, Batch 300, Loss: 1.58\n",
            "Epoch 35/50, Batch 400, Loss: 1.59\n",
            "Epoch 35/50, Batch 500, Loss: 1.49\n",
            "Epoch 35/50, Batch 600, Loss: 1.54\n",
            "Epoch 35/50, Batch 700, Loss: 1.53\n",
            "Epoch 35/50, Batch 800, Loss: 1.53\n",
            "Epoch 35/50, Batch 900, Loss: 1.55\n",
            "Epoch 36/50, Batch 100, Loss: 1.51\n",
            "Epoch 36/50, Batch 200, Loss: 1.48\n",
            "Epoch 36/50, Batch 300, Loss: 1.51\n",
            "Epoch 36/50, Batch 400, Loss: 1.62\n",
            "Epoch 36/50, Batch 500, Loss: 1.53\n",
            "Epoch 36/50, Batch 600, Loss: 1.49\n",
            "Epoch 36/50, Batch 700, Loss: 1.52\n",
            "Epoch 36/50, Batch 800, Loss: 1.52\n",
            "Epoch 36/50, Batch 900, Loss: 1.52\n",
            "Epoch 37/50, Batch 100, Loss: 1.50\n",
            "Epoch 37/50, Batch 200, Loss: 1.53\n",
            "Epoch 37/50, Batch 300, Loss: 1.54\n",
            "Epoch 37/50, Batch 400, Loss: 1.51\n",
            "Epoch 37/50, Batch 500, Loss: 1.51\n",
            "Epoch 37/50, Batch 600, Loss: 1.54\n",
            "Epoch 37/50, Batch 700, Loss: 1.57\n",
            "Epoch 37/50, Batch 800, Loss: 1.54\n",
            "Epoch 37/50, Batch 900, Loss: 1.52\n",
            "Epoch 38/50, Batch 100, Loss: 1.52\n",
            "Epoch 38/50, Batch 200, Loss: 1.49\n",
            "Epoch 38/50, Batch 300, Loss: 1.56\n",
            "Epoch 38/50, Batch 400, Loss: 1.50\n",
            "Epoch 38/50, Batch 500, Loss: 1.59\n",
            "Epoch 38/50, Batch 600, Loss: 1.53\n",
            "Epoch 38/50, Batch 700, Loss: 1.48\n",
            "Epoch 38/50, Batch 800, Loss: 1.50\n",
            "Epoch 38/50, Batch 900, Loss: 1.53\n",
            "Epoch 39/50, Batch 100, Loss: 1.56\n",
            "Epoch 39/50, Batch 200, Loss: 1.52\n",
            "Epoch 39/50, Batch 300, Loss: 1.53\n",
            "Epoch 39/50, Batch 400, Loss: 1.49\n",
            "Epoch 39/50, Batch 500, Loss: 1.51\n",
            "Epoch 39/50, Batch 600, Loss: 1.52\n",
            "Epoch 39/50, Batch 700, Loss: 1.51\n",
            "Epoch 39/50, Batch 800, Loss: 1.57\n",
            "Epoch 39/50, Batch 900, Loss: 1.53\n",
            "Epoch 40/50, Batch 100, Loss: 1.55\n",
            "Epoch 40/50, Batch 200, Loss: 1.61\n",
            "Epoch 40/50, Batch 300, Loss: 1.51\n",
            "Epoch 40/50, Batch 400, Loss: 1.54\n",
            "Epoch 40/50, Batch 500, Loss: 1.51\n",
            "Epoch 40/50, Batch 600, Loss: 1.48\n",
            "Epoch 40/50, Batch 700, Loss: 1.56\n",
            "Epoch 40/50, Batch 800, Loss: 1.53\n",
            "Epoch 40/50, Batch 900, Loss: 1.51\n",
            "Epoch 41/50, Batch 100, Loss: 1.59\n",
            "Epoch 41/50, Batch 200, Loss: 1.52\n",
            "Epoch 41/50, Batch 300, Loss: 1.56\n",
            "Epoch 41/50, Batch 400, Loss: 1.53\n",
            "Epoch 41/50, Batch 500, Loss: 1.56\n",
            "Epoch 41/50, Batch 600, Loss: 1.57\n",
            "Epoch 41/50, Batch 700, Loss: 1.51\n",
            "Epoch 41/50, Batch 800, Loss: 1.52\n",
            "Epoch 41/50, Batch 900, Loss: 1.49\n",
            "Epoch 42/50, Batch 100, Loss: 1.58\n",
            "Epoch 42/50, Batch 200, Loss: 1.55\n",
            "Epoch 42/50, Batch 300, Loss: 1.50\n",
            "Epoch 42/50, Batch 400, Loss: 1.54\n",
            "Epoch 42/50, Batch 500, Loss: 1.50\n",
            "Epoch 42/50, Batch 600, Loss: 1.57\n",
            "Epoch 42/50, Batch 700, Loss: 1.51\n",
            "Epoch 42/50, Batch 800, Loss: 1.56\n",
            "Epoch 42/50, Batch 900, Loss: 1.47\n",
            "Epoch 43/50, Batch 100, Loss: 1.52\n",
            "Epoch 43/50, Batch 200, Loss: 1.51\n",
            "Epoch 43/50, Batch 300, Loss: 1.52\n",
            "Epoch 43/50, Batch 400, Loss: 1.48\n",
            "Epoch 43/50, Batch 500, Loss: 1.52\n",
            "Epoch 43/50, Batch 600, Loss: 1.51\n",
            "Epoch 43/50, Batch 700, Loss: 1.51\n",
            "Epoch 43/50, Batch 800, Loss: 1.58\n",
            "Epoch 43/50, Batch 900, Loss: 1.53\n",
            "Epoch 44/50, Batch 100, Loss: 1.55\n",
            "Epoch 44/50, Batch 200, Loss: 1.52\n",
            "Epoch 44/50, Batch 300, Loss: 1.51\n",
            "Epoch 44/50, Batch 400, Loss: 1.49\n",
            "Epoch 44/50, Batch 500, Loss: 1.52\n",
            "Epoch 44/50, Batch 600, Loss: 1.56\n",
            "Epoch 44/50, Batch 700, Loss: 1.54\n",
            "Epoch 44/50, Batch 800, Loss: 1.51\n",
            "Epoch 44/50, Batch 900, Loss: 1.56\n",
            "Epoch 45/50, Batch 100, Loss: 1.53\n",
            "Epoch 45/50, Batch 200, Loss: 1.56\n",
            "Epoch 45/50, Batch 300, Loss: 1.54\n",
            "Epoch 45/50, Batch 400, Loss: 1.67\n",
            "Epoch 45/50, Batch 500, Loss: 1.54\n",
            "Epoch 45/50, Batch 600, Loss: 1.46\n",
            "Epoch 45/50, Batch 700, Loss: 1.58\n",
            "Epoch 45/50, Batch 800, Loss: 1.51\n",
            "Epoch 45/50, Batch 900, Loss: 1.57\n",
            "Epoch 46/50, Batch 100, Loss: 1.56\n",
            "Epoch 46/50, Batch 200, Loss: 1.52\n",
            "Epoch 46/50, Batch 300, Loss: 1.50\n",
            "Epoch 46/50, Batch 400, Loss: 1.53\n",
            "Epoch 46/50, Batch 500, Loss: 1.61\n",
            "Epoch 46/50, Batch 600, Loss: 1.52\n",
            "Epoch 46/50, Batch 700, Loss: 1.49\n",
            "Epoch 46/50, Batch 800, Loss: 1.50\n",
            "Epoch 46/50, Batch 900, Loss: 1.54\n",
            "Epoch 47/50, Batch 100, Loss: 1.55\n",
            "Epoch 47/50, Batch 200, Loss: 1.53\n",
            "Epoch 47/50, Batch 300, Loss: 1.49\n",
            "Epoch 47/50, Batch 400, Loss: 1.51\n",
            "Epoch 47/50, Batch 500, Loss: 1.50\n",
            "Epoch 47/50, Batch 600, Loss: 1.53\n",
            "Epoch 47/50, Batch 700, Loss: 1.51\n",
            "Epoch 47/50, Batch 800, Loss: 1.51\n",
            "Epoch 47/50, Batch 900, Loss: 1.52\n",
            "Epoch 48/50, Batch 100, Loss: 1.50\n",
            "Epoch 48/50, Batch 200, Loss: 1.51\n",
            "Epoch 48/50, Batch 300, Loss: 1.55\n",
            "Epoch 48/50, Batch 400, Loss: 1.55\n",
            "Epoch 48/50, Batch 500, Loss: 1.56\n",
            "Epoch 48/50, Batch 600, Loss: 1.52\n",
            "Epoch 48/50, Batch 700, Loss: 1.50\n",
            "Epoch 48/50, Batch 800, Loss: 1.50\n",
            "Epoch 48/50, Batch 900, Loss: 1.55\n",
            "Epoch 49/50, Batch 100, Loss: 1.48\n",
            "Epoch 49/50, Batch 200, Loss: 1.55\n",
            "Epoch 49/50, Batch 300, Loss: 1.51\n",
            "Epoch 49/50, Batch 400, Loss: 1.58\n",
            "Epoch 49/50, Batch 500, Loss: 1.51\n",
            "Epoch 49/50, Batch 600, Loss: 1.52\n",
            "Epoch 49/50, Batch 700, Loss: 1.52\n",
            "Epoch 49/50, Batch 800, Loss: 1.52\n",
            "Epoch 49/50, Batch 900, Loss: 1.53\n",
            "Epoch 50/50, Batch 100, Loss: 1.51\n",
            "Epoch 50/50, Batch 200, Loss: 1.50\n",
            "Epoch 50/50, Batch 300, Loss: 1.51\n",
            "Epoch 50/50, Batch 400, Loss: 1.58\n",
            "Epoch 50/50, Batch 500, Loss: 1.50\n",
            "Epoch 50/50, Batch 600, Loss: 1.49\n",
            "Epoch 50/50, Batch 700, Loss: 1.53\n",
            "Epoch 50/50, Batch 800, Loss: 1.54\n",
            "Epoch 50/50, Batch 900, Loss: 1.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(loader,model):\n",
        "  if loader.dataset.train:\n",
        "    print('Getting accuracy on training data.')\n",
        "  else:\n",
        "    print('Getting accuracy on testing data.')\n",
        "  n_corrects = 0\n",
        "  n_samples = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      #x = x.reshape(x.shape[0],-1)\n",
        "\n",
        "      scores = model(x)\n",
        "      _,y_pred = scores.max(1)\n",
        "      n_corrects += (y_pred == y).sum()\n",
        "      n_samples += y_pred.size(0)\n",
        "\n",
        "    print(f'We got {n_corrects}/{n_samples} correct. Accuracy = {float(n_corrects)/float(n_samples)*100.0:.2f}')\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "5MY8E8YvggzH"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(train_loader, model)\n",
        "get_accuracy(test_loader,model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7zy9KqJghVb",
        "outputId": "98d82036-ddfc-4eb3-c026-5b00dc427b7b"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting accuracy on training data.\n",
            "We got 56185/60000 correct. Accuracy = 93.64\n",
            "Getting accuracy on testing data.\n",
            "We got 8820/10000 correct. Accuracy = 88.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 dataset"
      ],
      "metadata": {
        "id": "XzT67OwBS2eN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP model"
      ],
      "metadata": {
        "id": "ftrX_JQLS2eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "2VVunVewS2eO"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 3072\n",
        "n_classes = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "n_epochs = 50"
      ],
      "metadata": {
        "id": "ShmKyBfkS2eO"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root='/content/drive/MyDrive/datasets/CIFAR10', train=True,transform=transforms,download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True)\n",
        "test_dataset = datasets.CIFAR10(root='/content/drive/MyDrive/datasets/CIFAR10', train=False,transform=transforms,download=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kYBGI-7S2eO",
        "outputId": "fa456688-09a2-4e90-cb00-e2a979e9d26a"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auRLx2_7S2eP",
        "outputId": "23a804cd-7cc1-48b4-903d-bc9aaa5cdad4"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['airplane',\n",
              " 'automobile',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'deer',\n",
              " 'dog',\n",
              " 'frog',\n",
              " 'horse',\n",
              " 'ship',\n",
              " 'truck']"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " image, label = train_dataset[0]\n",
        " plt.imshow(image.permute((1, 2, 0)))\n",
        " print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "zI-mxTEbS2eP",
        "outputId": "4bc03bb7-5cb2-4eaa-a194-c2a0348cf952"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, n_classes):\n",
        "    super().__init__()\n",
        "    self.input_layer = nn.Linear(input_size, 1024)\n",
        "    self.hidden_layer_1 = nn.Linear(1024,512)\n",
        "    self.hidden_layer_2 = nn.Linear(512,256)\n",
        "    self.hidden_layer_3 = nn.Linear(256,64)\n",
        "    self.output_layer = nn.Linear(64, n_classes)\n",
        "  \n",
        "  def forward(self, X):\n",
        "    X = self.input_layer(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.hidden_layer_1(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.hidden_layer_2(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.hidden_layer_3(X)\n",
        "    X = F.relu(X)\n",
        "    X = self.output_layer(X)\n",
        "    prob = F.softmax(X, dim = 1)\n",
        "    return prob"
      ],
      "metadata": {
        "id": "4lVH3gkoS2eP"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(input_size=input_size,n_classes=n_classes).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVd5-Hw1S2eQ",
        "outputId": "6f6b03eb-a01e-4a41-8910-651ad75ab96e"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (input_layer): Linear(in_features=3072, out_features=1024, bias=True)\n",
            "  (hidden_layer_1): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (hidden_layer_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (hidden_layer_3): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (output_layer): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "PpBRPf6TS2eQ"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "  for batch_idx, (data,targets) in enumerate(train_loader):\n",
        "    \n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "\n",
        "    data = data.reshape(data.shape[0],-1)\n",
        "\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch_idx + 1) % 100 == 0:\n",
        "      print(f'Epoch {epoch+1}/{n_epochs}, Batch {batch_idx+1}, Loss: {loss.item():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrixEPqgS2eQ",
        "outputId": "c9ed765b-da1b-4ba1-d1dc-b0df359fa472"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Batch 100, Loss: 2.23\n",
            "Epoch 1/50, Batch 200, Loss: 2.23\n",
            "Epoch 1/50, Batch 300, Loss: 2.19\n",
            "Epoch 1/50, Batch 400, Loss: 2.22\n",
            "Epoch 1/50, Batch 500, Loss: 2.21\n",
            "Epoch 1/50, Batch 600, Loss: 2.21\n",
            "Epoch 1/50, Batch 700, Loss: 2.18\n",
            "Epoch 2/50, Batch 100, Loss: 2.22\n",
            "Epoch 2/50, Batch 200, Loss: 2.11\n",
            "Epoch 2/50, Batch 300, Loss: 2.09\n",
            "Epoch 2/50, Batch 400, Loss: 2.20\n",
            "Epoch 2/50, Batch 500, Loss: 2.13\n",
            "Epoch 2/50, Batch 600, Loss: 2.00\n",
            "Epoch 2/50, Batch 700, Loss: 2.11\n",
            "Epoch 3/50, Batch 100, Loss: 2.14\n",
            "Epoch 3/50, Batch 200, Loss: 2.09\n",
            "Epoch 3/50, Batch 300, Loss: 2.10\n",
            "Epoch 3/50, Batch 400, Loss: 2.17\n",
            "Epoch 3/50, Batch 500, Loss: 2.14\n",
            "Epoch 3/50, Batch 600, Loss: 2.20\n",
            "Epoch 3/50, Batch 700, Loss: 2.15\n",
            "Epoch 4/50, Batch 100, Loss: 2.09\n",
            "Epoch 4/50, Batch 200, Loss: 2.13\n",
            "Epoch 4/50, Batch 300, Loss: 2.07\n",
            "Epoch 4/50, Batch 400, Loss: 2.17\n",
            "Epoch 4/50, Batch 500, Loss: 2.21\n",
            "Epoch 4/50, Batch 600, Loss: 2.08\n",
            "Epoch 4/50, Batch 700, Loss: 2.09\n",
            "Epoch 5/50, Batch 100, Loss: 2.08\n",
            "Epoch 5/50, Batch 200, Loss: 2.16\n",
            "Epoch 5/50, Batch 300, Loss: 2.14\n",
            "Epoch 5/50, Batch 400, Loss: 2.12\n",
            "Epoch 5/50, Batch 500, Loss: 2.06\n",
            "Epoch 5/50, Batch 600, Loss: 2.14\n",
            "Epoch 5/50, Batch 700, Loss: 2.16\n",
            "Epoch 6/50, Batch 100, Loss: 2.19\n",
            "Epoch 6/50, Batch 200, Loss: 2.02\n",
            "Epoch 6/50, Batch 300, Loss: 2.08\n",
            "Epoch 6/50, Batch 400, Loss: 2.17\n",
            "Epoch 6/50, Batch 500, Loss: 2.10\n",
            "Epoch 6/50, Batch 600, Loss: 2.10\n",
            "Epoch 6/50, Batch 700, Loss: 2.18\n",
            "Epoch 7/50, Batch 100, Loss: 2.05\n",
            "Epoch 7/50, Batch 200, Loss: 2.13\n",
            "Epoch 7/50, Batch 300, Loss: 2.06\n",
            "Epoch 7/50, Batch 400, Loss: 2.09\n",
            "Epoch 7/50, Batch 500, Loss: 2.08\n",
            "Epoch 7/50, Batch 600, Loss: 2.14\n",
            "Epoch 7/50, Batch 700, Loss: 2.09\n",
            "Epoch 8/50, Batch 100, Loss: 2.03\n",
            "Epoch 8/50, Batch 200, Loss: 2.02\n",
            "Epoch 8/50, Batch 300, Loss: 2.02\n",
            "Epoch 8/50, Batch 400, Loss: 1.99\n",
            "Epoch 8/50, Batch 500, Loss: 2.10\n",
            "Epoch 8/50, Batch 600, Loss: 2.07\n",
            "Epoch 8/50, Batch 700, Loss: 2.05\n",
            "Epoch 9/50, Batch 100, Loss: 2.10\n",
            "Epoch 9/50, Batch 200, Loss: 2.19\n",
            "Epoch 9/50, Batch 300, Loss: 1.99\n",
            "Epoch 9/50, Batch 400, Loss: 2.02\n",
            "Epoch 9/50, Batch 500, Loss: 2.07\n",
            "Epoch 9/50, Batch 600, Loss: 2.13\n",
            "Epoch 9/50, Batch 700, Loss: 2.11\n",
            "Epoch 10/50, Batch 100, Loss: 2.06\n",
            "Epoch 10/50, Batch 200, Loss: 2.07\n",
            "Epoch 10/50, Batch 300, Loss: 2.03\n",
            "Epoch 10/50, Batch 400, Loss: 1.99\n",
            "Epoch 10/50, Batch 500, Loss: 2.09\n",
            "Epoch 10/50, Batch 600, Loss: 2.10\n",
            "Epoch 10/50, Batch 700, Loss: 1.97\n",
            "Epoch 11/50, Batch 100, Loss: 2.09\n",
            "Epoch 11/50, Batch 200, Loss: 1.99\n",
            "Epoch 11/50, Batch 300, Loss: 2.09\n",
            "Epoch 11/50, Batch 400, Loss: 2.07\n",
            "Epoch 11/50, Batch 500, Loss: 2.10\n",
            "Epoch 11/50, Batch 600, Loss: 2.08\n",
            "Epoch 11/50, Batch 700, Loss: 1.99\n",
            "Epoch 12/50, Batch 100, Loss: 2.04\n",
            "Epoch 12/50, Batch 200, Loss: 1.99\n",
            "Epoch 12/50, Batch 300, Loss: 2.03\n",
            "Epoch 12/50, Batch 400, Loss: 2.07\n",
            "Epoch 12/50, Batch 500, Loss: 2.11\n",
            "Epoch 12/50, Batch 600, Loss: 2.11\n",
            "Epoch 12/50, Batch 700, Loss: 2.15\n",
            "Epoch 13/50, Batch 100, Loss: 2.06\n",
            "Epoch 13/50, Batch 200, Loss: 2.07\n",
            "Epoch 13/50, Batch 300, Loss: 2.15\n",
            "Epoch 13/50, Batch 400, Loss: 2.07\n",
            "Epoch 13/50, Batch 500, Loss: 2.13\n",
            "Epoch 13/50, Batch 600, Loss: 2.06\n",
            "Epoch 13/50, Batch 700, Loss: 1.93\n",
            "Epoch 14/50, Batch 100, Loss: 2.05\n",
            "Epoch 14/50, Batch 200, Loss: 2.03\n",
            "Epoch 14/50, Batch 300, Loss: 2.06\n",
            "Epoch 14/50, Batch 400, Loss: 2.11\n",
            "Epoch 14/50, Batch 500, Loss: 2.13\n",
            "Epoch 14/50, Batch 600, Loss: 1.92\n",
            "Epoch 14/50, Batch 700, Loss: 2.01\n",
            "Epoch 15/50, Batch 100, Loss: 2.11\n",
            "Epoch 15/50, Batch 200, Loss: 2.03\n",
            "Epoch 15/50, Batch 300, Loss: 2.15\n",
            "Epoch 15/50, Batch 400, Loss: 1.99\n",
            "Epoch 15/50, Batch 500, Loss: 2.02\n",
            "Epoch 15/50, Batch 600, Loss: 1.95\n",
            "Epoch 15/50, Batch 700, Loss: 2.10\n",
            "Epoch 16/50, Batch 100, Loss: 1.99\n",
            "Epoch 16/50, Batch 200, Loss: 2.05\n",
            "Epoch 16/50, Batch 300, Loss: 2.11\n",
            "Epoch 16/50, Batch 400, Loss: 2.05\n",
            "Epoch 16/50, Batch 500, Loss: 2.07\n",
            "Epoch 16/50, Batch 600, Loss: 1.99\n",
            "Epoch 16/50, Batch 700, Loss: 2.05\n",
            "Epoch 17/50, Batch 100, Loss: 2.09\n",
            "Epoch 17/50, Batch 200, Loss: 1.99\n",
            "Epoch 17/50, Batch 300, Loss: 1.95\n",
            "Epoch 17/50, Batch 400, Loss: 2.02\n",
            "Epoch 17/50, Batch 500, Loss: 2.00\n",
            "Epoch 17/50, Batch 600, Loss: 2.06\n",
            "Epoch 17/50, Batch 700, Loss: 2.00\n",
            "Epoch 18/50, Batch 100, Loss: 2.03\n",
            "Epoch 18/50, Batch 200, Loss: 2.01\n",
            "Epoch 18/50, Batch 300, Loss: 2.13\n",
            "Epoch 18/50, Batch 400, Loss: 2.05\n",
            "Epoch 18/50, Batch 500, Loss: 2.01\n",
            "Epoch 18/50, Batch 600, Loss: 2.01\n",
            "Epoch 18/50, Batch 700, Loss: 2.04\n",
            "Epoch 19/50, Batch 100, Loss: 2.06\n",
            "Epoch 19/50, Batch 200, Loss: 1.91\n",
            "Epoch 19/50, Batch 300, Loss: 2.10\n",
            "Epoch 19/50, Batch 400, Loss: 2.03\n",
            "Epoch 19/50, Batch 500, Loss: 2.08\n",
            "Epoch 19/50, Batch 600, Loss: 2.09\n",
            "Epoch 19/50, Batch 700, Loss: 2.05\n",
            "Epoch 20/50, Batch 100, Loss: 2.04\n",
            "Epoch 20/50, Batch 200, Loss: 2.10\n",
            "Epoch 20/50, Batch 300, Loss: 1.94\n",
            "Epoch 20/50, Batch 400, Loss: 2.03\n",
            "Epoch 20/50, Batch 500, Loss: 1.96\n",
            "Epoch 20/50, Batch 600, Loss: 2.06\n",
            "Epoch 20/50, Batch 700, Loss: 1.92\n",
            "Epoch 21/50, Batch 100, Loss: 1.92\n",
            "Epoch 21/50, Batch 200, Loss: 2.02\n",
            "Epoch 21/50, Batch 300, Loss: 2.04\n",
            "Epoch 21/50, Batch 400, Loss: 2.07\n",
            "Epoch 21/50, Batch 500, Loss: 2.01\n",
            "Epoch 21/50, Batch 600, Loss: 2.13\n",
            "Epoch 21/50, Batch 700, Loss: 2.03\n",
            "Epoch 22/50, Batch 100, Loss: 1.95\n",
            "Epoch 22/50, Batch 200, Loss: 2.02\n",
            "Epoch 22/50, Batch 300, Loss: 2.08\n",
            "Epoch 22/50, Batch 400, Loss: 2.02\n",
            "Epoch 22/50, Batch 500, Loss: 2.11\n",
            "Epoch 22/50, Batch 600, Loss: 2.00\n",
            "Epoch 22/50, Batch 700, Loss: 2.08\n",
            "Epoch 23/50, Batch 100, Loss: 2.06\n",
            "Epoch 23/50, Batch 200, Loss: 2.08\n",
            "Epoch 23/50, Batch 300, Loss: 2.03\n",
            "Epoch 23/50, Batch 400, Loss: 2.11\n",
            "Epoch 23/50, Batch 500, Loss: 2.02\n",
            "Epoch 23/50, Batch 600, Loss: 2.07\n",
            "Epoch 23/50, Batch 700, Loss: 2.12\n",
            "Epoch 24/50, Batch 100, Loss: 2.07\n",
            "Epoch 24/50, Batch 200, Loss: 2.00\n",
            "Epoch 24/50, Batch 300, Loss: 2.01\n",
            "Epoch 24/50, Batch 400, Loss: 2.16\n",
            "Epoch 24/50, Batch 500, Loss: 1.94\n",
            "Epoch 24/50, Batch 600, Loss: 1.99\n",
            "Epoch 24/50, Batch 700, Loss: 2.04\n",
            "Epoch 25/50, Batch 100, Loss: 1.99\n",
            "Epoch 25/50, Batch 200, Loss: 2.07\n",
            "Epoch 25/50, Batch 300, Loss: 2.00\n",
            "Epoch 25/50, Batch 400, Loss: 2.01\n",
            "Epoch 25/50, Batch 500, Loss: 2.18\n",
            "Epoch 25/50, Batch 600, Loss: 1.94\n",
            "Epoch 25/50, Batch 700, Loss: 2.07\n",
            "Epoch 26/50, Batch 100, Loss: 1.97\n",
            "Epoch 26/50, Batch 200, Loss: 2.06\n",
            "Epoch 26/50, Batch 300, Loss: 1.89\n",
            "Epoch 26/50, Batch 400, Loss: 1.95\n",
            "Epoch 26/50, Batch 500, Loss: 2.01\n",
            "Epoch 26/50, Batch 600, Loss: 2.02\n",
            "Epoch 26/50, Batch 700, Loss: 2.04\n",
            "Epoch 27/50, Batch 100, Loss: 1.96\n",
            "Epoch 27/50, Batch 200, Loss: 2.03\n",
            "Epoch 27/50, Batch 300, Loss: 2.05\n",
            "Epoch 27/50, Batch 400, Loss: 2.05\n",
            "Epoch 27/50, Batch 500, Loss: 2.11\n",
            "Epoch 27/50, Batch 600, Loss: 2.14\n",
            "Epoch 27/50, Batch 700, Loss: 2.13\n",
            "Epoch 28/50, Batch 100, Loss: 2.07\n",
            "Epoch 28/50, Batch 200, Loss: 2.06\n",
            "Epoch 28/50, Batch 300, Loss: 1.91\n",
            "Epoch 28/50, Batch 400, Loss: 1.99\n",
            "Epoch 28/50, Batch 500, Loss: 2.01\n",
            "Epoch 28/50, Batch 600, Loss: 2.05\n",
            "Epoch 28/50, Batch 700, Loss: 1.91\n",
            "Epoch 29/50, Batch 100, Loss: 1.98\n",
            "Epoch 29/50, Batch 200, Loss: 2.04\n",
            "Epoch 29/50, Batch 300, Loss: 2.03\n",
            "Epoch 29/50, Batch 400, Loss: 2.01\n",
            "Epoch 29/50, Batch 500, Loss: 1.99\n",
            "Epoch 29/50, Batch 600, Loss: 2.06\n",
            "Epoch 29/50, Batch 700, Loss: 1.99\n",
            "Epoch 30/50, Batch 100, Loss: 2.02\n",
            "Epoch 30/50, Batch 200, Loss: 1.98\n",
            "Epoch 30/50, Batch 300, Loss: 2.02\n",
            "Epoch 30/50, Batch 400, Loss: 1.99\n",
            "Epoch 30/50, Batch 500, Loss: 1.93\n",
            "Epoch 30/50, Batch 600, Loss: 2.07\n",
            "Epoch 30/50, Batch 700, Loss: 2.01\n",
            "Epoch 31/50, Batch 100, Loss: 2.03\n",
            "Epoch 31/50, Batch 200, Loss: 1.99\n",
            "Epoch 31/50, Batch 300, Loss: 2.02\n",
            "Epoch 31/50, Batch 400, Loss: 2.04\n",
            "Epoch 31/50, Batch 500, Loss: 2.01\n",
            "Epoch 31/50, Batch 600, Loss: 1.94\n",
            "Epoch 31/50, Batch 700, Loss: 2.09\n",
            "Epoch 32/50, Batch 100, Loss: 2.04\n",
            "Epoch 32/50, Batch 200, Loss: 1.96\n",
            "Epoch 32/50, Batch 300, Loss: 2.03\n",
            "Epoch 32/50, Batch 400, Loss: 2.05\n",
            "Epoch 32/50, Batch 500, Loss: 1.99\n",
            "Epoch 32/50, Batch 600, Loss: 2.01\n",
            "Epoch 32/50, Batch 700, Loss: 2.08\n",
            "Epoch 33/50, Batch 100, Loss: 2.07\n",
            "Epoch 33/50, Batch 200, Loss: 1.94\n",
            "Epoch 33/50, Batch 300, Loss: 2.11\n",
            "Epoch 33/50, Batch 400, Loss: 2.06\n",
            "Epoch 33/50, Batch 500, Loss: 2.04\n",
            "Epoch 33/50, Batch 600, Loss: 2.03\n",
            "Epoch 33/50, Batch 700, Loss: 1.97\n",
            "Epoch 34/50, Batch 100, Loss: 1.97\n",
            "Epoch 34/50, Batch 200, Loss: 2.03\n",
            "Epoch 34/50, Batch 300, Loss: 1.97\n",
            "Epoch 34/50, Batch 400, Loss: 2.00\n",
            "Epoch 34/50, Batch 500, Loss: 1.93\n",
            "Epoch 34/50, Batch 600, Loss: 2.05\n",
            "Epoch 34/50, Batch 700, Loss: 2.00\n",
            "Epoch 35/50, Batch 100, Loss: 2.03\n",
            "Epoch 35/50, Batch 200, Loss: 1.96\n",
            "Epoch 35/50, Batch 300, Loss: 2.06\n",
            "Epoch 35/50, Batch 400, Loss: 1.99\n",
            "Epoch 35/50, Batch 500, Loss: 2.03\n",
            "Epoch 35/50, Batch 600, Loss: 1.95\n",
            "Epoch 35/50, Batch 700, Loss: 2.03\n",
            "Epoch 36/50, Batch 100, Loss: 2.00\n",
            "Epoch 36/50, Batch 200, Loss: 2.03\n",
            "Epoch 36/50, Batch 300, Loss: 1.98\n",
            "Epoch 36/50, Batch 400, Loss: 2.01\n",
            "Epoch 36/50, Batch 500, Loss: 2.05\n",
            "Epoch 36/50, Batch 600, Loss: 1.97\n",
            "Epoch 36/50, Batch 700, Loss: 2.06\n",
            "Epoch 37/50, Batch 100, Loss: 2.05\n",
            "Epoch 37/50, Batch 200, Loss: 2.07\n",
            "Epoch 37/50, Batch 300, Loss: 2.08\n",
            "Epoch 37/50, Batch 400, Loss: 2.05\n",
            "Epoch 37/50, Batch 500, Loss: 1.95\n",
            "Epoch 37/50, Batch 600, Loss: 1.91\n",
            "Epoch 37/50, Batch 700, Loss: 1.99\n",
            "Epoch 38/50, Batch 100, Loss: 1.99\n",
            "Epoch 38/50, Batch 200, Loss: 2.07\n",
            "Epoch 38/50, Batch 300, Loss: 1.96\n",
            "Epoch 38/50, Batch 400, Loss: 1.87\n",
            "Epoch 38/50, Batch 500, Loss: 1.93\n",
            "Epoch 38/50, Batch 600, Loss: 2.07\n",
            "Epoch 38/50, Batch 700, Loss: 2.06\n",
            "Epoch 39/50, Batch 100, Loss: 1.98\n",
            "Epoch 39/50, Batch 200, Loss: 2.09\n",
            "Epoch 39/50, Batch 300, Loss: 2.03\n",
            "Epoch 39/50, Batch 400, Loss: 2.03\n",
            "Epoch 39/50, Batch 500, Loss: 1.98\n",
            "Epoch 39/50, Batch 600, Loss: 2.03\n",
            "Epoch 39/50, Batch 700, Loss: 2.06\n",
            "Epoch 40/50, Batch 100, Loss: 2.02\n",
            "Epoch 40/50, Batch 200, Loss: 1.97\n",
            "Epoch 40/50, Batch 300, Loss: 1.95\n",
            "Epoch 40/50, Batch 400, Loss: 1.99\n",
            "Epoch 40/50, Batch 500, Loss: 2.10\n",
            "Epoch 40/50, Batch 600, Loss: 2.00\n",
            "Epoch 40/50, Batch 700, Loss: 1.98\n",
            "Epoch 41/50, Batch 100, Loss: 2.15\n",
            "Epoch 41/50, Batch 200, Loss: 2.10\n",
            "Epoch 41/50, Batch 300, Loss: 1.97\n",
            "Epoch 41/50, Batch 400, Loss: 2.15\n",
            "Epoch 41/50, Batch 500, Loss: 2.12\n",
            "Epoch 41/50, Batch 600, Loss: 2.02\n",
            "Epoch 41/50, Batch 700, Loss: 2.19\n",
            "Epoch 42/50, Batch 100, Loss: 2.03\n",
            "Epoch 42/50, Batch 200, Loss: 2.06\n",
            "Epoch 42/50, Batch 300, Loss: 2.01\n",
            "Epoch 42/50, Batch 400, Loss: 1.98\n",
            "Epoch 42/50, Batch 500, Loss: 1.97\n",
            "Epoch 42/50, Batch 600, Loss: 2.05\n",
            "Epoch 42/50, Batch 700, Loss: 2.07\n",
            "Epoch 43/50, Batch 100, Loss: 2.10\n",
            "Epoch 43/50, Batch 200, Loss: 2.02\n",
            "Epoch 43/50, Batch 300, Loss: 2.08\n",
            "Epoch 43/50, Batch 400, Loss: 2.03\n",
            "Epoch 43/50, Batch 500, Loss: 2.08\n",
            "Epoch 43/50, Batch 600, Loss: 2.04\n",
            "Epoch 43/50, Batch 700, Loss: 2.08\n",
            "Epoch 44/50, Batch 100, Loss: 2.06\n",
            "Epoch 44/50, Batch 200, Loss: 1.97\n",
            "Epoch 44/50, Batch 300, Loss: 1.99\n",
            "Epoch 44/50, Batch 400, Loss: 2.11\n",
            "Epoch 44/50, Batch 500, Loss: 2.16\n",
            "Epoch 44/50, Batch 600, Loss: 2.12\n",
            "Epoch 44/50, Batch 700, Loss: 2.08\n",
            "Epoch 45/50, Batch 100, Loss: 2.09\n",
            "Epoch 45/50, Batch 200, Loss: 1.89\n",
            "Epoch 45/50, Batch 300, Loss: 1.94\n",
            "Epoch 45/50, Batch 400, Loss: 2.03\n",
            "Epoch 45/50, Batch 500, Loss: 2.00\n",
            "Epoch 45/50, Batch 600, Loss: 1.98\n",
            "Epoch 45/50, Batch 700, Loss: 2.02\n",
            "Epoch 46/50, Batch 100, Loss: 2.05\n",
            "Epoch 46/50, Batch 200, Loss: 2.08\n",
            "Epoch 46/50, Batch 300, Loss: 2.01\n",
            "Epoch 46/50, Batch 400, Loss: 2.10\n",
            "Epoch 46/50, Batch 500, Loss: 1.95\n",
            "Epoch 46/50, Batch 600, Loss: 2.04\n",
            "Epoch 46/50, Batch 700, Loss: 2.02\n",
            "Epoch 47/50, Batch 100, Loss: 2.09\n",
            "Epoch 47/50, Batch 200, Loss: 1.97\n",
            "Epoch 47/50, Batch 300, Loss: 1.93\n",
            "Epoch 47/50, Batch 400, Loss: 2.05\n",
            "Epoch 47/50, Batch 500, Loss: 2.06\n",
            "Epoch 47/50, Batch 600, Loss: 2.01\n",
            "Epoch 47/50, Batch 700, Loss: 2.09\n",
            "Epoch 48/50, Batch 100, Loss: 2.17\n",
            "Epoch 48/50, Batch 200, Loss: 2.03\n",
            "Epoch 48/50, Batch 300, Loss: 1.99\n",
            "Epoch 48/50, Batch 400, Loss: 2.08\n",
            "Epoch 48/50, Batch 500, Loss: 1.95\n",
            "Epoch 48/50, Batch 600, Loss: 2.00\n",
            "Epoch 48/50, Batch 700, Loss: 2.07\n",
            "Epoch 49/50, Batch 100, Loss: 2.00\n",
            "Epoch 49/50, Batch 200, Loss: 2.04\n",
            "Epoch 49/50, Batch 300, Loss: 2.08\n",
            "Epoch 49/50, Batch 400, Loss: 1.97\n",
            "Epoch 49/50, Batch 500, Loss: 2.07\n",
            "Epoch 49/50, Batch 600, Loss: 2.01\n",
            "Epoch 49/50, Batch 700, Loss: 1.94\n",
            "Epoch 50/50, Batch 100, Loss: 2.03\n",
            "Epoch 50/50, Batch 200, Loss: 2.02\n",
            "Epoch 50/50, Batch 300, Loss: 2.00\n",
            "Epoch 50/50, Batch 400, Loss: 2.02\n",
            "Epoch 50/50, Batch 500, Loss: 2.01\n",
            "Epoch 50/50, Batch 600, Loss: 2.10\n",
            "Epoch 50/50, Batch 700, Loss: 2.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(loader,model):\n",
        "  if loader.dataset.train:\n",
        "    print('Getting accuracy on training data.')\n",
        "  else:\n",
        "    print('Getting accuracy on testing data.')\n",
        "  n_corrects = 0\n",
        "  n_samples = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      x = x.reshape(x.shape[0],-1)\n",
        "\n",
        "      scores = model(x)\n",
        "      _,y_pred = scores.max(1)\n",
        "      n_corrects += (y_pred == y).sum()\n",
        "      n_samples += y_pred.size(0)\n",
        "\n",
        "    print(f'We got {n_corrects}/{n_samples} correct. Accuracy = {float(n_corrects)/float(n_samples)*100.0:.2f}')\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "2rzuCtxXS2eR"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(train_loader, model)\n",
        "get_accuracy(test_loader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O3RbQpRS2eR",
        "outputId": "f5202839-3eb3-4f33-f3fc-3ac00fdbe257"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting accuracy on training data.\n",
            "We got 20852/50000 correct. Accuracy = 41.70\n",
            "Getting accuracy on testing data.\n",
            "We got 4052/10000 correct. Accuracy = 40.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LeNet-5**"
      ],
      "metadata": {
        "id": "54Zv7n9LkqDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "n_epochs = 50"
      ],
      "metadata": {
        "id": "aksh0GShkqDF"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms = transforms.Compose([transforms.Resize((32,32)), transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "WDZIZeJYkqDF"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root='/content/drive/MyDrive/datasets/cifar10', train=True,transform=transforms,download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size, shuffle=True)\n",
        "test_dataset = datasets.CIFAR10(root='/content/drive/MyDrive/datasets/cifar10', train=False,transform=transforms,download=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKkTTLYnkqDF",
        "outputId": "b389f565-b7a2-477e-e109-aa940981bfce"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LaNet5(nn.Module):\n",
        "  def __init__(self, n_classes):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=6,kernel_size=5,stride=1),\n",
        "        nn.Tanh(),\n",
        "        nn.AvgPool2d(kernel_size=2),\n",
        "        nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5,stride=1),\n",
        "        nn.Tanh(),\n",
        "        nn.AvgPool2d(kernel_size=2),\n",
        "        nn.Conv2d(in_channels=16,out_channels=120,kernel_size=5,stride=1),\n",
        "        nn.Tanh(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=120,out_features=84),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(in_features=84,out_features=n_classes),\n",
        "        nn.Softmax(dim=1)\n",
        "        )\n",
        "  def forward(self, X):\n",
        "    prob = self.model(X)\n",
        "    return prob"
      ],
      "metadata": {
        "id": "a4Sway26kqDF"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LaNet5(n_classes=n_classes).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70875af5-e725-4f83-e0e3-04587b7dca2d",
        "id": "ZyKc8LSLkqDF"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LaNet5(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): Tanh()\n",
            "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): Tanh()\n",
            "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (6): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (7): Tanh()\n",
            "    (8): Flatten(start_dim=1, end_dim=-1)\n",
            "    (9): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (10): Tanh()\n",
            "    (11): Linear(in_features=84, out_features=10, bias=True)\n",
            "    (12): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "RwWnIcmgkqDH"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "  for batch_idx, (data,targets) in enumerate(train_loader):\n",
        "    \n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "\n",
        "    #data = data.reshape(data.shape[0],-1)\n",
        "\n",
        "    scores = model(data)\n",
        "    loss = criterion(scores, targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch_idx + 1) % 100 == 0:\n",
        "      print(f'Epoch {epoch+1}/{n_epochs}, Batch {batch_idx+1}, Loss: {loss.item():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5e3ef1-cf44-4995-fd76-d75ac4fbdfc3",
        "id": "V9S8GWSVkqDH"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Batch 100, Loss: 2.22\n",
            "Epoch 1/50, Batch 200, Loss: 2.14\n",
            "Epoch 1/50, Batch 300, Loss: 2.20\n",
            "Epoch 1/50, Batch 400, Loss: 2.12\n",
            "Epoch 1/50, Batch 500, Loss: 1.99\n",
            "Epoch 1/50, Batch 600, Loss: 2.11\n",
            "Epoch 1/50, Batch 700, Loss: 2.01\n",
            "Epoch 2/50, Batch 100, Loss: 2.12\n",
            "Epoch 2/50, Batch 200, Loss: 2.09\n",
            "Epoch 2/50, Batch 300, Loss: 2.03\n",
            "Epoch 2/50, Batch 400, Loss: 2.06\n",
            "Epoch 2/50, Batch 500, Loss: 2.10\n",
            "Epoch 2/50, Batch 600, Loss: 2.10\n",
            "Epoch 2/50, Batch 700, Loss: 2.13\n",
            "Epoch 3/50, Batch 100, Loss: 2.05\n",
            "Epoch 3/50, Batch 200, Loss: 1.99\n",
            "Epoch 3/50, Batch 300, Loss: 2.05\n",
            "Epoch 3/50, Batch 400, Loss: 2.07\n",
            "Epoch 3/50, Batch 500, Loss: 2.03\n",
            "Epoch 3/50, Batch 600, Loss: 2.08\n",
            "Epoch 3/50, Batch 700, Loss: 2.00\n",
            "Epoch 4/50, Batch 100, Loss: 2.02\n",
            "Epoch 4/50, Batch 200, Loss: 1.96\n",
            "Epoch 4/50, Batch 300, Loss: 1.90\n",
            "Epoch 4/50, Batch 400, Loss: 2.03\n",
            "Epoch 4/50, Batch 500, Loss: 2.00\n",
            "Epoch 4/50, Batch 600, Loss: 2.07\n",
            "Epoch 4/50, Batch 700, Loss: 2.06\n",
            "Epoch 5/50, Batch 100, Loss: 1.94\n",
            "Epoch 5/50, Batch 200, Loss: 2.00\n",
            "Epoch 5/50, Batch 300, Loss: 2.02\n",
            "Epoch 5/50, Batch 400, Loss: 1.95\n",
            "Epoch 5/50, Batch 500, Loss: 2.00\n",
            "Epoch 5/50, Batch 600, Loss: 2.08\n",
            "Epoch 5/50, Batch 700, Loss: 1.95\n",
            "Epoch 6/50, Batch 100, Loss: 1.94\n",
            "Epoch 6/50, Batch 200, Loss: 2.05\n",
            "Epoch 6/50, Batch 300, Loss: 2.02\n",
            "Epoch 6/50, Batch 400, Loss: 2.03\n",
            "Epoch 6/50, Batch 500, Loss: 2.04\n",
            "Epoch 6/50, Batch 600, Loss: 2.03\n",
            "Epoch 6/50, Batch 700, Loss: 2.00\n",
            "Epoch 7/50, Batch 100, Loss: 1.97\n",
            "Epoch 7/50, Batch 200, Loss: 1.89\n",
            "Epoch 7/50, Batch 300, Loss: 1.94\n",
            "Epoch 7/50, Batch 400, Loss: 1.99\n",
            "Epoch 7/50, Batch 500, Loss: 1.88\n",
            "Epoch 7/50, Batch 600, Loss: 1.92\n",
            "Epoch 7/50, Batch 700, Loss: 1.93\n",
            "Epoch 8/50, Batch 100, Loss: 1.91\n",
            "Epoch 8/50, Batch 200, Loss: 1.97\n",
            "Epoch 8/50, Batch 300, Loss: 2.02\n",
            "Epoch 8/50, Batch 400, Loss: 2.00\n",
            "Epoch 8/50, Batch 500, Loss: 1.96\n",
            "Epoch 8/50, Batch 600, Loss: 1.93\n",
            "Epoch 8/50, Batch 700, Loss: 1.95\n",
            "Epoch 9/50, Batch 100, Loss: 1.91\n",
            "Epoch 9/50, Batch 200, Loss: 1.95\n",
            "Epoch 9/50, Batch 300, Loss: 1.90\n",
            "Epoch 9/50, Batch 400, Loss: 1.93\n",
            "Epoch 9/50, Batch 500, Loss: 1.93\n",
            "Epoch 9/50, Batch 600, Loss: 1.89\n",
            "Epoch 9/50, Batch 700, Loss: 1.90\n",
            "Epoch 10/50, Batch 100, Loss: 1.97\n",
            "Epoch 10/50, Batch 200, Loss: 2.01\n",
            "Epoch 10/50, Batch 300, Loss: 1.95\n",
            "Epoch 10/50, Batch 400, Loss: 1.99\n",
            "Epoch 10/50, Batch 500, Loss: 1.88\n",
            "Epoch 10/50, Batch 600, Loss: 1.95\n",
            "Epoch 10/50, Batch 700, Loss: 1.90\n",
            "Epoch 11/50, Batch 100, Loss: 1.87\n",
            "Epoch 11/50, Batch 200, Loss: 1.75\n",
            "Epoch 11/50, Batch 300, Loss: 1.96\n",
            "Epoch 11/50, Batch 400, Loss: 1.93\n",
            "Epoch 11/50, Batch 500, Loss: 2.02\n",
            "Epoch 11/50, Batch 600, Loss: 1.80\n",
            "Epoch 11/50, Batch 700, Loss: 1.85\n",
            "Epoch 12/50, Batch 100, Loss: 1.89\n",
            "Epoch 12/50, Batch 200, Loss: 1.94\n",
            "Epoch 12/50, Batch 300, Loss: 1.87\n",
            "Epoch 12/50, Batch 400, Loss: 1.98\n",
            "Epoch 12/50, Batch 500, Loss: 1.99\n",
            "Epoch 12/50, Batch 600, Loss: 1.85\n",
            "Epoch 12/50, Batch 700, Loss: 1.86\n",
            "Epoch 13/50, Batch 100, Loss: 1.87\n",
            "Epoch 13/50, Batch 200, Loss: 1.94\n",
            "Epoch 13/50, Batch 300, Loss: 1.89\n",
            "Epoch 13/50, Batch 400, Loss: 1.95\n",
            "Epoch 13/50, Batch 500, Loss: 2.05\n",
            "Epoch 13/50, Batch 600, Loss: 2.01\n",
            "Epoch 13/50, Batch 700, Loss: 1.93\n",
            "Epoch 14/50, Batch 100, Loss: 1.87\n",
            "Epoch 14/50, Batch 200, Loss: 1.99\n",
            "Epoch 14/50, Batch 300, Loss: 1.98\n",
            "Epoch 14/50, Batch 400, Loss: 1.86\n",
            "Epoch 14/50, Batch 500, Loss: 1.85\n",
            "Epoch 14/50, Batch 600, Loss: 1.84\n",
            "Epoch 14/50, Batch 700, Loss: 1.83\n",
            "Epoch 15/50, Batch 100, Loss: 1.86\n",
            "Epoch 15/50, Batch 200, Loss: 1.79\n",
            "Epoch 15/50, Batch 300, Loss: 1.96\n",
            "Epoch 15/50, Batch 400, Loss: 1.85\n",
            "Epoch 15/50, Batch 500, Loss: 1.84\n",
            "Epoch 15/50, Batch 600, Loss: 1.95\n",
            "Epoch 15/50, Batch 700, Loss: 1.83\n",
            "Epoch 16/50, Batch 100, Loss: 1.91\n",
            "Epoch 16/50, Batch 200, Loss: 1.97\n",
            "Epoch 16/50, Batch 300, Loss: 1.87\n",
            "Epoch 16/50, Batch 400, Loss: 1.86\n",
            "Epoch 16/50, Batch 500, Loss: 1.88\n",
            "Epoch 16/50, Batch 600, Loss: 1.85\n",
            "Epoch 16/50, Batch 700, Loss: 1.79\n",
            "Epoch 17/50, Batch 100, Loss: 1.79\n",
            "Epoch 17/50, Batch 200, Loss: 1.80\n",
            "Epoch 17/50, Batch 300, Loss: 1.86\n",
            "Epoch 17/50, Batch 400, Loss: 1.84\n",
            "Epoch 17/50, Batch 500, Loss: 1.90\n",
            "Epoch 17/50, Batch 600, Loss: 1.90\n",
            "Epoch 17/50, Batch 700, Loss: 1.75\n",
            "Epoch 18/50, Batch 100, Loss: 1.89\n",
            "Epoch 18/50, Batch 200, Loss: 1.94\n",
            "Epoch 18/50, Batch 300, Loss: 1.91\n",
            "Epoch 18/50, Batch 400, Loss: 1.87\n",
            "Epoch 18/50, Batch 500, Loss: 1.92\n",
            "Epoch 18/50, Batch 600, Loss: 1.92\n",
            "Epoch 18/50, Batch 700, Loss: 1.87\n",
            "Epoch 19/50, Batch 100, Loss: 1.82\n",
            "Epoch 19/50, Batch 200, Loss: 1.89\n",
            "Epoch 19/50, Batch 300, Loss: 1.88\n",
            "Epoch 19/50, Batch 400, Loss: 1.81\n",
            "Epoch 19/50, Batch 500, Loss: 1.89\n",
            "Epoch 19/50, Batch 600, Loss: 1.87\n",
            "Epoch 19/50, Batch 700, Loss: 1.92\n",
            "Epoch 20/50, Batch 100, Loss: 1.83\n",
            "Epoch 20/50, Batch 200, Loss: 1.86\n",
            "Epoch 20/50, Batch 300, Loss: 1.80\n",
            "Epoch 20/50, Batch 400, Loss: 1.89\n",
            "Epoch 20/50, Batch 500, Loss: 1.77\n",
            "Epoch 20/50, Batch 600, Loss: 1.85\n",
            "Epoch 20/50, Batch 700, Loss: 1.79\n",
            "Epoch 21/50, Batch 100, Loss: 1.88\n",
            "Epoch 21/50, Batch 200, Loss: 1.94\n",
            "Epoch 21/50, Batch 300, Loss: 1.88\n",
            "Epoch 21/50, Batch 400, Loss: 1.88\n",
            "Epoch 21/50, Batch 500, Loss: 1.92\n",
            "Epoch 21/50, Batch 600, Loss: 1.89\n",
            "Epoch 21/50, Batch 700, Loss: 1.92\n",
            "Epoch 22/50, Batch 100, Loss: 1.77\n",
            "Epoch 22/50, Batch 200, Loss: 1.85\n",
            "Epoch 22/50, Batch 300, Loss: 1.82\n",
            "Epoch 22/50, Batch 400, Loss: 1.90\n",
            "Epoch 22/50, Batch 500, Loss: 1.82\n",
            "Epoch 22/50, Batch 600, Loss: 1.75\n",
            "Epoch 22/50, Batch 700, Loss: 1.94\n",
            "Epoch 23/50, Batch 100, Loss: 1.82\n",
            "Epoch 23/50, Batch 200, Loss: 1.80\n",
            "Epoch 23/50, Batch 300, Loss: 1.97\n",
            "Epoch 23/50, Batch 400, Loss: 1.83\n",
            "Epoch 23/50, Batch 500, Loss: 1.96\n",
            "Epoch 23/50, Batch 600, Loss: 1.78\n",
            "Epoch 23/50, Batch 700, Loss: 1.76\n",
            "Epoch 24/50, Batch 100, Loss: 1.75\n",
            "Epoch 24/50, Batch 200, Loss: 1.79\n",
            "Epoch 24/50, Batch 300, Loss: 1.82\n",
            "Epoch 24/50, Batch 400, Loss: 1.71\n",
            "Epoch 24/50, Batch 500, Loss: 1.88\n",
            "Epoch 24/50, Batch 600, Loss: 1.91\n",
            "Epoch 24/50, Batch 700, Loss: 1.83\n",
            "Epoch 25/50, Batch 100, Loss: 1.87\n",
            "Epoch 25/50, Batch 200, Loss: 1.79\n",
            "Epoch 25/50, Batch 300, Loss: 1.85\n",
            "Epoch 25/50, Batch 400, Loss: 1.76\n",
            "Epoch 25/50, Batch 500, Loss: 1.93\n",
            "Epoch 25/50, Batch 600, Loss: 1.81\n",
            "Epoch 25/50, Batch 700, Loss: 1.91\n",
            "Epoch 26/50, Batch 100, Loss: 1.90\n",
            "Epoch 26/50, Batch 200, Loss: 1.88\n",
            "Epoch 26/50, Batch 300, Loss: 1.75\n",
            "Epoch 26/50, Batch 400, Loss: 1.76\n",
            "Epoch 26/50, Batch 500, Loss: 1.83\n",
            "Epoch 26/50, Batch 600, Loss: 1.81\n",
            "Epoch 26/50, Batch 700, Loss: 1.76\n",
            "Epoch 27/50, Batch 100, Loss: 1.81\n",
            "Epoch 27/50, Batch 200, Loss: 1.91\n",
            "Epoch 27/50, Batch 300, Loss: 1.78\n",
            "Epoch 27/50, Batch 400, Loss: 1.84\n",
            "Epoch 27/50, Batch 500, Loss: 1.85\n",
            "Epoch 27/50, Batch 600, Loss: 1.82\n",
            "Epoch 27/50, Batch 700, Loss: 1.90\n",
            "Epoch 28/50, Batch 100, Loss: 1.90\n",
            "Epoch 28/50, Batch 200, Loss: 1.93\n",
            "Epoch 28/50, Batch 300, Loss: 1.88\n",
            "Epoch 28/50, Batch 400, Loss: 1.86\n",
            "Epoch 28/50, Batch 500, Loss: 1.85\n",
            "Epoch 28/50, Batch 600, Loss: 1.77\n",
            "Epoch 28/50, Batch 700, Loss: 1.87\n",
            "Epoch 29/50, Batch 100, Loss: 1.82\n",
            "Epoch 29/50, Batch 200, Loss: 1.80\n",
            "Epoch 29/50, Batch 300, Loss: 1.77\n",
            "Epoch 29/50, Batch 400, Loss: 1.74\n",
            "Epoch 29/50, Batch 500, Loss: 1.84\n",
            "Epoch 29/50, Batch 600, Loss: 1.80\n",
            "Epoch 29/50, Batch 700, Loss: 1.93\n",
            "Epoch 30/50, Batch 100, Loss: 1.83\n",
            "Epoch 30/50, Batch 200, Loss: 1.73\n",
            "Epoch 30/50, Batch 300, Loss: 1.78\n",
            "Epoch 30/50, Batch 400, Loss: 1.84\n",
            "Epoch 30/50, Batch 500, Loss: 1.73\n",
            "Epoch 30/50, Batch 600, Loss: 1.77\n",
            "Epoch 30/50, Batch 700, Loss: 1.85\n",
            "Epoch 31/50, Batch 100, Loss: 1.76\n",
            "Epoch 31/50, Batch 200, Loss: 1.85\n",
            "Epoch 31/50, Batch 300, Loss: 1.82\n",
            "Epoch 31/50, Batch 400, Loss: 1.88\n",
            "Epoch 31/50, Batch 500, Loss: 1.80\n",
            "Epoch 31/50, Batch 600, Loss: 1.82\n",
            "Epoch 31/50, Batch 700, Loss: 1.92\n",
            "Epoch 32/50, Batch 100, Loss: 1.76\n",
            "Epoch 32/50, Batch 200, Loss: 1.77\n",
            "Epoch 32/50, Batch 300, Loss: 1.92\n",
            "Epoch 32/50, Batch 400, Loss: 1.75\n",
            "Epoch 32/50, Batch 500, Loss: 1.88\n",
            "Epoch 32/50, Batch 600, Loss: 1.85\n",
            "Epoch 32/50, Batch 700, Loss: 1.78\n",
            "Epoch 33/50, Batch 100, Loss: 1.74\n",
            "Epoch 33/50, Batch 200, Loss: 1.81\n",
            "Epoch 33/50, Batch 300, Loss: 1.88\n",
            "Epoch 33/50, Batch 400, Loss: 1.81\n",
            "Epoch 33/50, Batch 500, Loss: 1.81\n",
            "Epoch 33/50, Batch 600, Loss: 1.83\n",
            "Epoch 33/50, Batch 700, Loss: 1.82\n",
            "Epoch 34/50, Batch 100, Loss: 1.77\n",
            "Epoch 34/50, Batch 200, Loss: 1.80\n",
            "Epoch 34/50, Batch 300, Loss: 1.65\n",
            "Epoch 34/50, Batch 400, Loss: 1.80\n",
            "Epoch 34/50, Batch 500, Loss: 1.88\n",
            "Epoch 34/50, Batch 600, Loss: 1.78\n",
            "Epoch 34/50, Batch 700, Loss: 1.74\n",
            "Epoch 35/50, Batch 100, Loss: 1.83\n",
            "Epoch 35/50, Batch 200, Loss: 1.79\n",
            "Epoch 35/50, Batch 300, Loss: 1.90\n",
            "Epoch 35/50, Batch 400, Loss: 1.76\n",
            "Epoch 35/50, Batch 500, Loss: 1.80\n",
            "Epoch 35/50, Batch 600, Loss: 1.87\n",
            "Epoch 35/50, Batch 700, Loss: 1.79\n",
            "Epoch 36/50, Batch 100, Loss: 1.71\n",
            "Epoch 36/50, Batch 200, Loss: 1.74\n",
            "Epoch 36/50, Batch 300, Loss: 1.79\n",
            "Epoch 36/50, Batch 400, Loss: 1.86\n",
            "Epoch 36/50, Batch 500, Loss: 1.86\n",
            "Epoch 36/50, Batch 600, Loss: 1.73\n",
            "Epoch 36/50, Batch 700, Loss: 1.76\n",
            "Epoch 37/50, Batch 100, Loss: 1.83\n",
            "Epoch 37/50, Batch 200, Loss: 1.79\n",
            "Epoch 37/50, Batch 300, Loss: 1.69\n",
            "Epoch 37/50, Batch 400, Loss: 1.71\n",
            "Epoch 37/50, Batch 500, Loss: 1.85\n",
            "Epoch 37/50, Batch 600, Loss: 1.85\n",
            "Epoch 37/50, Batch 700, Loss: 1.79\n",
            "Epoch 38/50, Batch 100, Loss: 1.77\n",
            "Epoch 38/50, Batch 200, Loss: 1.75\n",
            "Epoch 38/50, Batch 300, Loss: 1.77\n",
            "Epoch 38/50, Batch 400, Loss: 1.77\n",
            "Epoch 38/50, Batch 500, Loss: 1.76\n",
            "Epoch 38/50, Batch 600, Loss: 1.81\n",
            "Epoch 38/50, Batch 700, Loss: 1.72\n",
            "Epoch 39/50, Batch 100, Loss: 1.81\n",
            "Epoch 39/50, Batch 200, Loss: 1.79\n",
            "Epoch 39/50, Batch 300, Loss: 1.79\n",
            "Epoch 39/50, Batch 400, Loss: 1.77\n",
            "Epoch 39/50, Batch 500, Loss: 1.69\n",
            "Epoch 39/50, Batch 600, Loss: 1.84\n",
            "Epoch 39/50, Batch 700, Loss: 1.88\n",
            "Epoch 40/50, Batch 100, Loss: 1.67\n",
            "Epoch 40/50, Batch 200, Loss: 1.75\n",
            "Epoch 40/50, Batch 300, Loss: 1.79\n",
            "Epoch 40/50, Batch 400, Loss: 1.81\n",
            "Epoch 40/50, Batch 500, Loss: 1.71\n",
            "Epoch 40/50, Batch 600, Loss: 1.70\n",
            "Epoch 40/50, Batch 700, Loss: 1.74\n",
            "Epoch 41/50, Batch 100, Loss: 1.74\n",
            "Epoch 41/50, Batch 200, Loss: 1.87\n",
            "Epoch 41/50, Batch 300, Loss: 1.85\n",
            "Epoch 41/50, Batch 400, Loss: 1.76\n",
            "Epoch 41/50, Batch 500, Loss: 1.71\n",
            "Epoch 41/50, Batch 600, Loss: 1.79\n",
            "Epoch 41/50, Batch 700, Loss: 1.76\n",
            "Epoch 42/50, Batch 100, Loss: 1.86\n",
            "Epoch 42/50, Batch 200, Loss: 1.74\n",
            "Epoch 42/50, Batch 300, Loss: 1.82\n",
            "Epoch 42/50, Batch 400, Loss: 1.78\n",
            "Epoch 42/50, Batch 500, Loss: 1.74\n",
            "Epoch 42/50, Batch 600, Loss: 1.87\n",
            "Epoch 42/50, Batch 700, Loss: 1.80\n",
            "Epoch 43/50, Batch 100, Loss: 1.76\n",
            "Epoch 43/50, Batch 200, Loss: 1.72\n",
            "Epoch 43/50, Batch 300, Loss: 1.89\n",
            "Epoch 43/50, Batch 400, Loss: 1.73\n",
            "Epoch 43/50, Batch 500, Loss: 1.86\n",
            "Epoch 43/50, Batch 600, Loss: 1.82\n",
            "Epoch 43/50, Batch 700, Loss: 1.82\n",
            "Epoch 44/50, Batch 100, Loss: 1.69\n",
            "Epoch 44/50, Batch 200, Loss: 1.70\n",
            "Epoch 44/50, Batch 300, Loss: 1.76\n",
            "Epoch 44/50, Batch 400, Loss: 1.72\n",
            "Epoch 44/50, Batch 500, Loss: 1.76\n",
            "Epoch 44/50, Batch 600, Loss: 1.79\n",
            "Epoch 44/50, Batch 700, Loss: 1.69\n",
            "Epoch 45/50, Batch 100, Loss: 1.71\n",
            "Epoch 45/50, Batch 200, Loss: 1.76\n",
            "Epoch 45/50, Batch 300, Loss: 1.79\n",
            "Epoch 45/50, Batch 400, Loss: 1.75\n",
            "Epoch 45/50, Batch 500, Loss: 1.81\n",
            "Epoch 45/50, Batch 600, Loss: 1.68\n",
            "Epoch 45/50, Batch 700, Loss: 1.74\n",
            "Epoch 46/50, Batch 100, Loss: 1.72\n",
            "Epoch 46/50, Batch 200, Loss: 1.73\n",
            "Epoch 46/50, Batch 300, Loss: 1.65\n",
            "Epoch 46/50, Batch 400, Loss: 1.76\n",
            "Epoch 46/50, Batch 500, Loss: 1.78\n",
            "Epoch 46/50, Batch 600, Loss: 1.84\n",
            "Epoch 46/50, Batch 700, Loss: 1.80\n",
            "Epoch 47/50, Batch 100, Loss: 1.70\n",
            "Epoch 47/50, Batch 200, Loss: 1.61\n",
            "Epoch 47/50, Batch 300, Loss: 1.73\n",
            "Epoch 47/50, Batch 400, Loss: 1.76\n",
            "Epoch 47/50, Batch 500, Loss: 1.80\n",
            "Epoch 47/50, Batch 600, Loss: 1.80\n",
            "Epoch 47/50, Batch 700, Loss: 1.75\n",
            "Epoch 48/50, Batch 100, Loss: 1.77\n",
            "Epoch 48/50, Batch 200, Loss: 1.82\n",
            "Epoch 48/50, Batch 300, Loss: 1.77\n",
            "Epoch 48/50, Batch 400, Loss: 1.74\n",
            "Epoch 48/50, Batch 500, Loss: 1.71\n",
            "Epoch 48/50, Batch 600, Loss: 1.73\n",
            "Epoch 48/50, Batch 700, Loss: 1.83\n",
            "Epoch 49/50, Batch 100, Loss: 1.81\n",
            "Epoch 49/50, Batch 200, Loss: 1.78\n",
            "Epoch 49/50, Batch 300, Loss: 1.70\n",
            "Epoch 49/50, Batch 400, Loss: 1.72\n",
            "Epoch 49/50, Batch 500, Loss: 1.74\n",
            "Epoch 49/50, Batch 600, Loss: 1.74\n",
            "Epoch 49/50, Batch 700, Loss: 1.68\n",
            "Epoch 50/50, Batch 100, Loss: 1.78\n",
            "Epoch 50/50, Batch 200, Loss: 1.73\n",
            "Epoch 50/50, Batch 300, Loss: 1.97\n",
            "Epoch 50/50, Batch 400, Loss: 1.70\n",
            "Epoch 50/50, Batch 500, Loss: 1.76\n",
            "Epoch 50/50, Batch 600, Loss: 1.74\n",
            "Epoch 50/50, Batch 700, Loss: 1.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(loader,model):\n",
        "  if loader.dataset.train:\n",
        "    print('Getting accuracy on training data.')\n",
        "  else:\n",
        "    print('Getting accuracy on testing data.')\n",
        "  n_corrects = 0\n",
        "  n_samples = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      #x = x.reshape(x.shape[0],-1)\n",
        "\n",
        "      scores = model(x)\n",
        "      _,y_pred = scores.max(1)\n",
        "      n_corrects += (y_pred == y).sum()\n",
        "      n_samples += y_pred.size(0)\n",
        "\n",
        "    print(f'We got {n_corrects}/{n_samples} correct. Accuracy = {float(n_corrects)/float(n_samples)*100.0:.2f}')\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "YzBwK9wHkqDH"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(train_loader, model)\n",
        "get_accuracy(test_loader,model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0c06ed-f473-41fb-ea2b-bec12a7df82d",
        "id": "BAI1COZ4kqDH"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting accuracy on training data.\n",
            "We got 35944/50000 correct. Accuracy = 71.89\n",
            "Getting accuracy on testing data.\n",
            "We got 5099/10000 correct. Accuracy = 50.99\n"
          ]
        }
      ]
    }
  ]
}